<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>Apache Accumulo™</title>
    <description>The Apache Accumulo™ sorted, distributed key/value store is a robust, scalable, high performance data storage and retrieval system.
</description>
    <link>https://accumulo.apache.org/</link>
    <atom:link href="https://accumulo.apache.org/feed.xml" rel="self" type="application/rss+xml"/>
    <pubDate>Fri, 25 Mar 2022 16:21:36 +0000</pubDate>
    <lastBuildDate>Fri, 25 Mar 2022 16:21:36 +0000</lastBuildDate>
    <generator>Jekyll v4.2.0</generator>
    
    
      <item>
        <title>Apache Accumulo 1.10.2</title>
        <description>&lt;h2 id=&quot;about&quot;&gt;About&lt;/h2&gt;

&lt;p&gt;Apache Accumulo 1.10.2 is a bug fix release of the 1.10 LTM release line.&lt;/p&gt;

&lt;p&gt;These release notes are highlights of the changes since 1.10.1. The full
detailed changes can be seen in the git history. If anything important is
missing from this list, please &lt;a href=&quot;/contact-us&quot;&gt;contact&lt;/a&gt; us to have it included.&lt;/p&gt;

&lt;p&gt;Users of 1.10.1 or earlier are encouraged to upgrade to 1.10.2, as this is a
continuation of the 1.10 LTM release line with bug fixes and improvements, and
it supersedes any prior 1.x version. Users are also encouraged to consider
migrating to a 2.x version when one that is suitable for their needs becomes
available.&lt;/p&gt;

&lt;h2 id=&quot;major-improvements&quot;&gt;Major Improvements&lt;/h2&gt;

&lt;p&gt;This release bundles &lt;a href=&quot;https://reload4j.qos.ch/&quot;&gt;reload4j&lt;/a&gt; (&lt;a href=&quot;https://github.com/apache/accumulo/issues/2458&quot;&gt;#2458&lt;/a&gt;) in
the convenience binary and uses that instead of log4j 1.2. This is to make it
easier for users to avoid the many CVEs that apply to log4j 1.2, which is no
longer being maintained. Accumulo 2.x versions will have already switched to
use the latest log4j 2. However, doing so required making some breaking API
changes and other substantial changes, so that can’t be done for Accumulo 1.10.
Using reload4j instead, was deemed to be a viable interim solution until
Accumulo 2.x.&lt;/p&gt;

&lt;h3 id=&quot;other-improvements&quot;&gt;Other Improvements&lt;/h3&gt;

&lt;ul&gt;
  &lt;li&gt;&lt;a href=&quot;https://github.com/apache/accumulo/issues/1808&quot;&gt;#1808&lt;/a&gt; Re-throw exceptions in threads instead of merely logging them&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;https://github.com/apache/accumulo/issues/1863&quot;&gt;#1863&lt;/a&gt; Avoid unnecessory redundant log sorting&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;https://github.com/apache/accumulo/issues/1917&quot;&gt;#1917&lt;/a&gt; Ensure RFileWriterBuilder API validates filenames&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;https://github.com/apache/accumulo/issues/2006&quot;&gt;#2006&lt;/a&gt; Detect system config changes in HostRegexTableLoadBalancer without restarting master&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;https://github.com/apache/accumulo/issues/2464&quot;&gt;#2464&lt;/a&gt; Apply timeout to socket.connect()&lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&quot;other-bug-fixes&quot;&gt;Other Bug Fixes&lt;/h3&gt;

&lt;ul&gt;
  &lt;li&gt;&lt;a href=&quot;https://github.com/apache/accumulo/issues/1775&quot;&gt;#1775&lt;/a&gt; Ensure monitor reports a dead tserver when it is killed&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;https://github.com/apache/accumulo/issues/1858&quot;&gt;#1858&lt;/a&gt; Fix a bug in the monitor graphs due to use of int instead of long&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;https://github.com/apache/accumulo/issues/2370&quot;&gt;#2370&lt;/a&gt; Fix bug in getsplits command in the shell&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;note-about-jdk-15&quot;&gt;Note About JDK 15&lt;/h2&gt;

&lt;p&gt;See the note in the 1.10.1 release notes about the use of JDK 15 or later, as
the information pertaining to the use of the CMS garbage collector remains
applicable to this version.&lt;/p&gt;

&lt;h2 id=&quot;useful-links&quot;&gt;Useful Links&lt;/h2&gt;

&lt;ul&gt;
  &lt;li&gt;&lt;a href=&quot;https://lists.apache.org/thread/bq424vnov27nwnkb471oxg5nd7m6xwn9&quot;&gt;Release VOTE email thread&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;https://github.com/apache/accumulo/compare/rel/1.10.1...apache:rel/1.10.2&quot;&gt;All Changes since 1.10.1&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;https://github.com/apache/accumulo/issues?q=project%3Aapache%2Faccumulo%2F18&quot;&gt;GitHub&lt;/a&gt; - List of issues tracked on GitHub corresponding to this release&lt;/li&gt;
&lt;/ul&gt;

</description>
        <pubDate>Sun, 13 Feb 2022 00:00:00 +0000</pubDate>
        <link>https://accumulo.apache.org/release/accumulo-1.10.2/</link>
        <guid isPermaLink="true">https://accumulo.apache.org/release/accumulo-1.10.2/</guid>
        
        
        <category>release</category>
        
      </item>
    
      <item>
        <title>External Compactions</title>
        <description>&lt;p&gt;External compactions are a new feature in Accumulo 2.1.0 which allows
compaction work to run outside of Tablet Servers.&lt;/p&gt;

&lt;h2 id=&quot;overview&quot;&gt;Overview&lt;/h2&gt;

&lt;p&gt;There are two types of &lt;a href=&quot;https://storage.googleapis.com/pub-tools-public-publication-data/pdf/68a74a85e1662fe02ff3967497f31fda7f32225c.pdf&quot;&gt;compactions&lt;/a&gt; in Accumulo - Minor and Major. Minor
compactions flush recently written data from memory to a new file. Major
compactions merge two or more Tablet files together into one new file. Starting
in 2.1 Tablet Servers can run multiple major compactions for a Tablet
concurrently; there is no longer a single thread pool per Tablet Server that
runs compactions. Major compactions can be resource intensive and may run for a
long time depending on several factors, to include the number and size of the
input files, and the iterators configured to run during major compaction.
Additionally, the Tablet Server does not currently have a mechanism in place to
stop a major compaction that is taking too long or using too many resources.
There is a mechanism to throttle the read and write speed of major compactions
as a way to reduce the resource contention on a Tablet Server where many
concurrent compactions are running. However, throttling compactions on a busy
system will just lead to an increasing amount of queued compactions. Finally,
major compaction work can be wasted in the event of an untimely death of the
Tablet Server or if a Tablet is migrated to another Tablet Server.&lt;/p&gt;

&lt;p&gt;An external compaction is a major compaction that occurs outside of a Tablet
Server. The external compaction feature is an extension of the major compaction
service in the Tablet Server and is configured as part of the systems
compaction service configuration. Thus, it is an optional feature. The goal of
the external compaction feature is to overcome some of the drawbacks of the
Major compactions that happen inside the Tablet Server. Specifically, external
compactions:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;Allow major compactions to continue when the originating TabletServer dies&lt;/li&gt;
  &lt;li&gt;Allow major compactions to occur while a Tablet migrates to a new Tablet Server&lt;/li&gt;
  &lt;li&gt;Reduce the load on the TabletServer, giving it more cycles to insert mutations and respond to scans (assuming it’s running on different hosts).  MapReduce jobs and compactions can lower the effectiveness of processor and page caches for scans, so moving compactions off the host can be beneficial.&lt;/li&gt;
  &lt;li&gt;Allow major compactions to be scaled differently than the number of TabletServers, giving users more flexibility in allocating resources.&lt;/li&gt;
  &lt;li&gt;Even out hotspots where a few Tablet Servers have a lot of compaction work. External compactions allow this work to spread much wider than previously possible.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;The external compaction feature in Apache Accumulo version 2.1.0 adds two new
system-level processes and new configuration properties. The new system-level
processes are the Compactor and the Compaction Coordinator.&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;The Compactor is a process that is responsible for executing a major compaction. There can be many Compactor’s running on a system. The Compactor communicates with the Compaction Coordinator to get information about the next major compaction it will run and to report the completion state.&lt;/li&gt;
  &lt;li&gt;The Compaction Coordinator is a single process like the Manager. It is responsible for communicating with the Tablet Servers to gather information about queued external compactions, to reserve a major compaction on the Compactor’s behalf, and to report the completion status of the reserved major compaction.  For external compactions that complete when the Tablet is offline, the Compaction Coordinator buffers this information and reports it later.&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;details&quot;&gt;Details&lt;/h2&gt;

&lt;p&gt;Before we explain the implementation for external compactions, it’s probably
useful to explain the changes for major compactions that were made in the 2.1.0
branch before external compactions were added. This is most apparent in the
&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;tserver.compaction.major.service&lt;/code&gt; and &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;table.compaction.dispatcher&lt;/code&gt; configuration
properties. The simplest way to explain this is that you can now define a
service for executing compactions and then assign that service to a table
(which implies you can have multiple services assigned to different tables).
This gives the flexibility to prevent one table’s compactions from impacting
another table. Each service has named thread pools with size thresholds.&lt;/p&gt;

&lt;h3 id=&quot;configuration&quot;&gt;Configuration&lt;/h3&gt;

&lt;p&gt;The configuration below defines a compaction service named cs1 using
the DefaultCompactionPlanner that is configured to have three named thread
pools (small, medium, and large). Each thread pool is configured with a number
of threads to run compactions and a size threshold. If the sum of the input
file sizes is less than 16MB, then the major compaction will be assigned to the
small pool, for example.&lt;/p&gt;

&lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;tserver.compaction.major.service.cs1.planner=org.apache.accumulo.core.spi.compaction.DefaultCompactionPlanner
tserver.compaction.major.service.cs1.planner.opts.executors=[
{&quot;name&quot;:&quot;small&quot;,&quot;type&quot;:&quot;internal&quot;,&quot;maxSize&quot;:&quot;16M&quot;,&quot;numThreads&quot;:8},
{&quot;name&quot;:&quot;medium&quot;,&quot;type&quot;:&quot;internal&quot;,&quot;maxSize&quot;:&quot;128M&quot;,&quot;numThreads&quot;:4},
{&quot;name&quot;:&quot;large&quot;,&quot;type&quot;:&quot;internal&quot;,&quot;numThreads&quot;:2}]
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;To assign compaction service cs1 to the table ci, you would use the following properties:&lt;/p&gt;

&lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;config -t ci -s table.compaction.dispatcher=org.apache.accumulo.core.spi.compaction.SimpleCompactionDispatcher
config -t ci -s table.compaction.dispatcher.opts.service=cs1
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;A small modification to the
tserver.compaction.major.service.cs1.planner.opts.executors property in the
example above would enable it to use external compactions. For example, let’s
say that we wanted all of the large compactions to be done externally, you
would use this configuration:&lt;/p&gt;

&lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;tserver.compaction.major.service.cs1.planner.opts.executors=[
{&quot;name&quot;:&quot;small&quot;,&quot;type&quot;:&quot;internal&quot;,&quot;maxSize&quot;:&quot;16M&quot;,&quot;numThreads&quot;:8},
{&quot;name&quot;:&quot;medium&quot;,&quot;type&quot;:&quot;internal&quot;,&quot;maxSize&quot;:&quot;128M&quot;,&quot;numThreads&quot;:4},
{&quot;name&quot;:&quot;large&quot;,&quot;type&quot;:&quot;external&quot;,&quot;queue&quot;:&quot;DCQ1&quot;}]'
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;In this example the queue DCQ1 can be any arbitrary name and allows you to
define multiple pools of Compactor’s.&lt;/p&gt;

&lt;p&gt;Behind these new configurations in 2.1 lies a new algorithm for choosing which
files to compact.  This algorithm attempts to find the smallest set of files
that meets the compaction ratio criteria. Prior to 2.1, Accumulo looked for the
largest set of files that met the criteria.  Both algorithms do logarithmic
amounts of work.  The new algorithm better utilizes multiple thread pools
available for running comactions of different sizes.&lt;/p&gt;

&lt;h3 id=&quot;compactor&quot;&gt;Compactor&lt;/h3&gt;

&lt;p&gt;A Compactor is started with the name of the queue for which it will complete
major compactions. You pass in the queue name when starting the Compactor, like
so:&lt;/p&gt;

&lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;bin/accumulo compactor -q DCQ1
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;Once started the Compactor tries to find the location of the
Compaction Coordinator in ZooKeeper and connect to it. Then, it asks the
Compaction Coordinator for the next compaction job for the queue. The
Compaction Coordinator will return to the Compactor the necessary information to
run the major compaction, assuming there is work to be done. Note that the
class performing the major compaction in the Compactor is the same one used in
the Tablet Server, so we are just transferring all of the input parameters from
the Tablet Server to the Compactor. The Compactor communicates information back
to the Compaction Coordinator when the compaction has started, finished
(successfully or not), and during the compaction (progress updates).&lt;/p&gt;

&lt;h3 id=&quot;compaction-coordinator&quot;&gt;Compaction Coordinator&lt;/h3&gt;

&lt;p&gt;The Compaction Coordinator is a singleton process in the system like the
Manager. Also, like the Manager it supports standby Compaction Coordinator’s
using locks in ZooKeeper. The Compaction Coordinator is started using the
command:&lt;/p&gt;

&lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;bin/accumulo compaction-coordinator
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;When running, the Compaction Coordinator polls the TabletServers for summary
information about their external compaction queues. It keeps track of the major
compaction priorities for each Tablet Server and queue. When a Compactor
requests the next major compaction job the Compaction Coordinator finds the
Tablet Server with the highest priority major compaction for that queue and
communicates with that Tablet Server to reserve an external compaction. The
priority in this case is an integer value based on the number of input files
for the compaction. For system compactions, the number is negative starting at
-32768 and increasing to -1 and for user compactions it’s a non-negative number
starting at 0 and limited to 32767. When the Tablet Server reserves the
external compaction an entry is written into the metadata table row for the
Tablet with the address of the Compactor running the compaction and all of the
configuration information passed back from the Tablet Server. Below is an
example of the ecomp metadata column:&lt;/p&gt;

&lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;2;10ba2e8ba2e8ba5 ecomp:ECID:94db8374-8275-4f89-ba8b-4c6b3908bc50 []    {&quot;inputs&quot;:[&quot;hdfs://accucluster/accumulo/tables/2/t-00000ur/A00001y9.rf&quot;,&quot;hdfs://accucluster/accumulo/tables/2/t-00000ur/C00005lp.rf&quot;,&quot;hdfs://accucluster/accumulo/tables/2/t-00000ur/F0000dqm.rf&quot;,&quot;hdfs://accucluster/accumulo/tables/2/t-00000ur/F0000dq1.rf&quot;],&quot;nextFiles&quot;:[],&quot;tmp&quot;:&quot;hdfs://accucluster/accumulo/tables/2/t-00000ur/C0000dqs.rf_tmp&quot;,&quot;compactor&quot;:&quot;10.2.0.139:9133&quot;,&quot;kind&quot;:&quot;SYSTEM&quot;,&quot;executorId&quot;:&quot;DCQ1&quot;,&quot;priority&quot;:-32754,&quot;propDels&quot;:true,&quot;selectedAll&quot;:false}
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;When the Compactor notifies the Compaction Coordinator that it has finished the
major compaction, the Compaction Coordinator attempts to notify the Tablet
Server and inserts an external compaction final state marker into the metadata
table. Below is an example of the final state marker:&lt;/p&gt;

&lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;~ecompECID:de6afc1d-64ae-4abf-8bce-02ec0a79aa6c : []        {&quot;extent&quot;:{&quot;tableId&quot;:&quot;2&quot;},&quot;state&quot;:&quot;FINISHED&quot;,&quot;fileSize&quot;:12354,&quot;entries&quot;:100000}
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;If the Compaction Coordinator is able to reach the Tablet Server and that Tablet
Server is still hosting the Tablet, then the compaction is committed and both
of the entries are removed from the metadata table. In the case that the Tablet
is offline when the compaction attempts to commit, there is a thread in the
Compaction Coordinator that looks for completed, but not yet committed, external
compactions and periodically attempts to contact the Tablet Server hosting the
Tablet to commit the compaction. The Compaction Coordinator periodically removes
the final state markers related to Tablets that no longer exist. In the case of
an external compaction failure the Compaction Coordinator notifies the Tablet
and the Tablet cleans up file reservations and removes the metadata entry.&lt;/p&gt;

&lt;h3 id=&quot;edge-cases&quot;&gt;Edge Cases&lt;/h3&gt;

&lt;p&gt;There are several situations involving external compactions that we tested as part of this feature. These are:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;Tablet migration&lt;/li&gt;
  &lt;li&gt;When a user initiated compaction is canceled&lt;/li&gt;
  &lt;li&gt;What a Table is taken offline&lt;/li&gt;
  &lt;li&gt;When a Tablet is split or merged&lt;/li&gt;
  &lt;li&gt;Coordinator restart&lt;/li&gt;
  &lt;li&gt;Tablet Server death&lt;/li&gt;
  &lt;li&gt;Table deletion&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;Compactors periodically check if the compaction they are running is related to
a deleted table, split/merged Tablet, or canceled user initiated compaction. If
any of these cases happen the Compactor interrupts the compaction and notifies
the Compaction Coordinator. An external compaction continues in the case of
Tablet Server death, Tablet migration, Coordinator restart, and the Table being
taken offline.&lt;/p&gt;

&lt;h2 id=&quot;cluster-test&quot;&gt;Cluster Test&lt;/h2&gt;

&lt;p&gt;The following tests were run on a cluster to exercise this new feature.&lt;/p&gt;

&lt;ol&gt;
  &lt;li&gt;Run continuous ingest for 24h with large compactions running externally in an autoscaled Kubernetes cluster.&lt;/li&gt;
  &lt;li&gt;After ingest completion, started a full table compaction with all compactions running externally.&lt;/li&gt;
  &lt;li&gt;Run continuous ingest verification process that looks for lost data.&lt;/li&gt;
&lt;/ol&gt;

&lt;h3 id=&quot;setup&quot;&gt;Setup&lt;/h3&gt;

&lt;p&gt;For these tests Accumulo, Zookeeper, and HDFS were run on a cluster in Azure
setup by Muchos and external compactions were run in a separate Kubernetes
cluster running in Azure.  The Accumulo cluster had the following
configuration.&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;Centos 7&lt;/li&gt;
  &lt;li&gt;Open JDK 11&lt;/li&gt;
  &lt;li&gt;Zookeeper 3.6.2&lt;/li&gt;
  &lt;li&gt;Hadoop 3.3.0&lt;/li&gt;
  &lt;li&gt;Accumulo 2.1.0-SNAPSHOT &lt;a href=&quot;https://github.com/apache/accumulo/commit/dad7e01ae7d450064cba5d60a1e0770311ebdb64&quot;&gt;dad7e01&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;23 D16s_v4 VMs, each with 16x128G HDDs stripped using LVM. 22 were workers.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;The following diagram shows how the two clusters were setup.  The Muchos and
Kubernetes clusters were on the same private vnet, each with its own /16 subnet
in the 10.x.x.x IP address space.  The Kubernetes cluster that ran external
compactions was backed by at least 3 D8s_v4 VMs, with VMs autoscaling with the
number of pods running.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/images/blog/202107_ecomp/clusters-layout.png&quot; alt=&quot;Cluster Layout&quot; /&gt;&lt;/p&gt;

&lt;p&gt;One problem we ran into was communication between Compactors running inside
Kubernetes with processes like the Compaction Coordinator and DataNodes running
outside of Kubernetes in the Muchos cluster.  For some insights into how these
problems were overcome, checkout the comments in the &lt;a href=&quot;/images/blog/202107_ecomp/accumulo-compactor-muchos.yaml&quot;&gt;deployment
spec&lt;/a&gt; used.&lt;/p&gt;

&lt;h3 id=&quot;configuration-1&quot;&gt;Configuration&lt;/h3&gt;

&lt;p&gt;The following Accumulo shell commands set up a new compaction service named
cs1.  This compaction service has an internal executor with 4 threads named
small for compactions less than 32M, an internal executor with 2 threads named
medium for compactions less than 128M, and an external compaction queue named
DCQ1 for all other compactions.&lt;/p&gt;

&lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;config -s 'tserver.compaction.major.service.cs1.planner.opts.executors=[{&quot;name&quot;:&quot;small&quot;,&quot;type&quot;:&quot;internal&quot;,&quot;maxSize&quot;:&quot;32M&quot;,&quot;numThreads&quot;:4},{&quot;name&quot;:&quot;medium&quot;,&quot;type&quot;:&quot;internal&quot;,&quot;maxSize&quot;:&quot;128M&quot;,&quot;numThreads&quot;:2},{&quot;name&quot;:&quot;large&quot;,&quot;type&quot;:&quot;external&quot;,&quot;queue&quot;:&quot;DCQ1&quot;}]'
config -s tserver.compaction.major.service.cs1.planner=org.apache.accumulo.core.spi.compaction.DefaultCompactionPlanner
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;The continuous ingest table was configured to use the above compaction service.
The table’s compaction ratio was also lowered from the default of 3 to 2.  A
lower compaction ratio results in less files per Tablet and more compaction
work.&lt;/p&gt;

&lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;config -t ci -s table.compaction.dispatcher=org.apache.accumulo.core.spi.compaction.SimpleCompactionDispatcher
config -t ci -s table.compaction.dispatcher.opts.service=cs1
config -t ci -s table.compaction.major.ratio=2
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;The Compaction Coordinator was manually started on the Muchos VM where the
Accumulo Manager, Zookeeper server, and the Namenode were running. The
following command was used to do this.&lt;/p&gt;

&lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;nohup accumulo compaction-coordinator &amp;gt;/var/data/logs/accumulo/compaction-coordinator.out 2&amp;gt;/var/data/logs/accumulo/compaction-coordinator.err &amp;amp;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;To start Compactors, Accumulo’s
&lt;a href=&quot;https://github.com/apache/accumulo-docker/tree/next-release&quot;&gt;docker&lt;/a&gt; image was
built from the &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;next-release&lt;/code&gt; branch by checking out the Apache Accumulo git
repo at commit &lt;a href=&quot;https://github.com/apache/accumulo/commit/dad7e01ae7d450064cba5d60a1e0770311ebdb64&quot;&gt;dad7e01&lt;/a&gt; and building the binary distribution using the
command &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;mvn clean package -DskipTests&lt;/code&gt;. The resulting tar file was copied to
the accumulo-docker base directory and the image was built using the command:&lt;/p&gt;

&lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;docker build --build-arg ACCUMULO_VERSION=2.1.0-SNAPSHOT --build-arg ACCUMULO_FILE=accumulo-2.1.0-SNAPSHOT-bin.tar.gz \
             --build-arg HADOOP_FILE=hadoop-3.3.0.tar.gz \
             --build-arg ZOOKEEPER_VERSION=3.6.2  --build-arg ZOOKEEPER_FILE=apache-zookeeper-3.6.2-bin.tar.gz  \
             -t accumulo .
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;The Docker image was tagged and then pushed to a container registry accessible by
Kubernetes. Then the following commands were run to start the Compactors using
&lt;a href=&quot;/images/blog/202107_ecomp/accumulo-compactor-muchos.yaml&quot;&gt;accumulo-compactor-muchos.yaml&lt;/a&gt;.
The yaml file contains comments explaining issues related to IP addresses and DNS names.&lt;/p&gt;

&lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;kubectl apply -f accumulo-compactor-muchos.yaml 
kubectl autoscale deployment accumulo-compactor --cpu-percent=80 --min=10 --max=660
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;The autoscale command causes Compactors to scale between 10
and 660 pods based on CPU usage. When pods average CPU is above 80%, then
pods are added to meet the 80% goal. When it’s below 80%, pods
are stopped to meet the 80% goal with 5 minutes between scale down
events. This can sometimes lead to running compactions being
stopped. During the test there were ~537 dead compactions that were probably
caused by this (there were 44K successful external compactions). The max of 660
was chosen based on the number of datanodes in the Muchos cluster.  There were
22 datanodes and 30x22=660, so this conceptually sets a limit of 30 external
compactions per datanode.  This was well tolerated by the Muchos cluster.  One
important lesson we learned is that external compactions can strain the HDFS
DataNodes, so it’s important to consider how many concurrent external
compactions will be running. The Muchos cluster had 22x16=352 cores on the
worker VMs, so the max of 660 exceeds what the Muchos cluster could run itself.&lt;/p&gt;

&lt;h3 id=&quot;ingesting-data&quot;&gt;Ingesting data&lt;/h3&gt;

&lt;p&gt;After starting Compactors, 22 continuous ingest clients (from
accumulo_testing) were started.  The following plot shows the number of
compactions running in the three different compaction queues
configured.  The executor cs1_small is for compactions &amp;lt;= 32M and it stayed
pretty busy as minor compactions constantly produce new small files.  In 2.1.0
merging minor compactions were removed, so it’s important to ensure a
compaction queue is properly configured for new small files. The executor
cs1_medium was for compactions &amp;gt;32M and &amp;lt;=128M and it was not as busy, but did
have steady work.  The external compaction queue DCQ1 processed all compactions
over 128M and had some spikes of work.  These spikes are to be expected with
continuous ingest as all Tablets are written to evenly and eventually all of
the Tablets need to run large compactions around the same time.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/images/blog/202107_ecomp/ci-running.png&quot; alt=&quot;Compactions Running&quot; /&gt;&lt;/p&gt;

&lt;p&gt;The following plot shows the number of pods running in Kubernetes.  As
Compactors used more and less CPU the number of pods automatically scaled up
and down.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/images/blog/202107_ecomp/ci-pods-running.png&quot; alt=&quot;Pods Running&quot; /&gt;&lt;/p&gt;

&lt;p&gt;The following plot shows the number of compactions queued.  When the
compactions queued for cs1_small spiked above 750, it was adjusted from 4
threads per Tablet Server to 6 threads.  This configuration change was made while
everything was running and the Tablet Servers saw it and reconfigured their thread
pools on the fly.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/images/blog/202107_ecomp/ci-queued.png&quot; alt=&quot;Pods Queued&quot; /&gt;&lt;/p&gt;

&lt;p&gt;The metrics emitted by Accumulo for these plots had the following names.&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;TabletServer1.tserver.compactionExecutors.e_DCQ1_queued&lt;/li&gt;
  &lt;li&gt;TabletServer1.tserver.compactionExecutors.e_DCQ1_running&lt;/li&gt;
  &lt;li&gt;TabletServer1.tserver.compactionExecutors.i_cs1_medium_queued&lt;/li&gt;
  &lt;li&gt;TabletServer1.tserver.compactionExecutors.i_cs1_medium_running&lt;/li&gt;
  &lt;li&gt;TabletServer1.tserver.compactionExecutors.i_cs1_small_queued&lt;/li&gt;
  &lt;li&gt;TabletServer1.tserver.compactionExecutors.i_cs1_small_running&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;Tablet servers emit metrics about queued and running compactions for every
compaction executor configured.  User can observe these metrics and tune
the configuration based on what they see, as was done in this test.&lt;/p&gt;

&lt;p&gt;The following plot shows the average files per Tablet during the
test. The numbers are what would be expected for a compaction ratio of 2 when
the system is keeping up with compaction work. Also, animated GIFs were created to
show a few tablets &lt;a href=&quot;/images/blog/202107_ecomp/files_over_time.html&quot;&gt;files over time&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/images/blog/202107_ecomp/ci-files-per-tablet.png&quot; alt=&quot;Files Per Tablet&quot; /&gt;&lt;/p&gt;

&lt;p&gt;The following is a plot of the number Tablets during the test.
Eventually there were 11.28K Tablets around 512 Tablets per Tablet Server.  The
Tablets were close to splitting again at the end of the test as each Tablet was
getting close to 1G.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/images/blog/202107_ecomp/ci-online-tablets.png&quot; alt=&quot;Online Tablets&quot; /&gt;&lt;/p&gt;

&lt;p&gt;The following plot shows ingest rate over time.  The rate goes down as the
number of Tablets per Tablet Server goes up, this is expected.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/images/blog/202107_ecomp/ci-ingest-rate.png&quot; alt=&quot;Ingest Rate&quot; /&gt;&lt;/p&gt;

&lt;p&gt;The following plot shows the number of key/values in Accumulo during
the test.  When ingest was stopped, there were 266 billion key values in the
continuous ingest table.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/images/blog/202107_ecomp/ci-entries.png&quot; alt=&quot;Table Entries&quot; /&gt;&lt;/p&gt;

&lt;h3 id=&quot;full-table-compaction&quot;&gt;Full table compaction&lt;/h3&gt;

&lt;p&gt;After stopping ingest and letting things settle, a full table compaction was
kicked off. Since all of these compactions would be over 128M, all of them were
scheduled on the external queue DCQ1.  The two plots below show compactions
running and queued for the ~2 hours it took to do the compaction. When the
compaction was initiated there were 10 Compactors running in pods.  All 11K
Tablets were queued for compaction and because the pods were always running
high CPU Kubernetes kept adding pods until the max was reached resulting in 660
Compactors running until all the work was done.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/images/blog/202107_ecomp/full-table-compaction-queued.png&quot; alt=&quot;Full Table Compactions Running&quot; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/images/blog/202107_ecomp/full-table-compaction-running.png&quot; alt=&quot;Full Table Compactions Queued&quot; /&gt;&lt;/p&gt;

&lt;h3 id=&quot;verification&quot;&gt;Verification&lt;/h3&gt;

&lt;p&gt;After running everything mentioned above, the continuous ingest verification
map reduce job was run.  This job looks for holes in the linked list produced
by continuous ingest which indicate Accumulo lost data.  No holes were found.
The counts below were emitted by the job.  If there were holes a non-zero
UNDEFINED count would be present.&lt;/p&gt;

&lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;        org.apache.accumulo.testing.continuous.ContinuousVerify$Counts
                REFERENCED=266225036149
                UNREFERENCED=22010637
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;h2 id=&quot;hurdles&quot;&gt;Hurdles&lt;/h2&gt;

&lt;h3 id=&quot;how-to-scale-up&quot;&gt;How to Scale Up&lt;/h3&gt;

&lt;p&gt;We ran into several issues running the Compactors in Kubernetes. First, we knew
that we could use Kubernetes &lt;a href=&quot;https://kubernetes.io/docs/tasks/run-application/horizontal-pod-autoscale/&quot;&gt;Horizontal Pod Autoscaler&lt;/a&gt; (HPA) to scale the
Compactors up and down based on load. But the question remained how to do that.
Probably the best metric to use for scaling the Compactors is the size of the
external compaction queue. Another possible solution is to take the DataNode
utilization into account somehow. We found that in scaling up the Compactors
based on their CPU usage we could overload DataNodes.  Once DataNodes were
overwhelmed, Compactors CPU would drop and the number of pods would naturally
scale down.&lt;/p&gt;

&lt;p&gt;To use custom metrics you would need to get the metrics from Accumulo into a
metrics store that has a &lt;a href=&quot;https://github.com/kubernetes/metrics/blob/master/IMPLEMENTATIONS.md#custom-metrics-api&quot;&gt;metrics adapter&lt;/a&gt;. One possible solution, available
in Hadoop 3.3.0, is to use Prometheus, the &lt;a href=&quot;https://github.com/kubernetes-sigs/prometheus-adapter&quot;&gt;Prometheus Adapter&lt;/a&gt;, and enable
the Hadoop PrometheusMetricsSink added in 
&lt;a href=&quot;https://issues.apache.org/jira/browse/HADOOP-16398&quot;&gt;HADOOP-16398&lt;/a&gt; to expose the custom queue
size metrics. This seemed like the right solution, but it also seemed like a
lot of work that was outside the scope of this blog post. Ultimately we decided
to take the simplest approach - use the native Kubernetes metrics-server and
scale off CPU usage of the Compactors. As you can see in the “Compactions Queued”
and “Compactions Running” graphs above from the full table compaction, it took about
45 minutes for Kubernetes to scale up Compactors to the maximum configured (660). Compactors
likely would have been scaled up much faster if scaling was done off the queued compactions
instead of CPU usage.&lt;/p&gt;

&lt;h3 id=&quot;gracefully-scaling-down&quot;&gt;Gracefully Scaling Down&lt;/h3&gt;

&lt;p&gt;The Kubernetes Pod &lt;a href=&quot;https://kubernetes.io/docs/concepts/workloads/pods/pod-lifecycle/#pod-termination&quot;&gt;termination process&lt;/a&gt; provides a mechanism for the user to
define a pre-stop hook that will be called before the Pod is terminated.
Without this hook Kubernetes sends a SIGTERM to the Pod, followed by a
user-defined grace period, then a SIGKILL. For the purposes of this test we did
not define a pre-stop hook or a grace period. It’s likely possible to handle
this situation more gracefully, but for this test our Compactors were killed
and the compaction work lost when the HPA decided to scale down the Compactors.
It was a good test of how we handled failed Compactors.  Investigation is
needed to determine if changes are needed in Accumulo to facilitate graceful
scale down.&lt;/p&gt;

&lt;h3 id=&quot;how-to-connect&quot;&gt;How to Connect&lt;/h3&gt;

&lt;p&gt;The other major issue we ran into was connectivity between the Compactors and
the other server processes. The Compactor communicates with ZooKeeper and the
Compaction Coordinator, both of which were running outside of Kubernetes.  There
is no common DNS between the Muchos and Kubernetes cluster, but IPs were
visible to both. The Compactor connects to ZooKeeper to find the address of the
Compaction Coordinator so that it can connect to it and look for work. By
default the Accumulo server processes use the hostname as their address which
would not work as those names would not resolve inside the Kubernetes cluster.
We had to start the Accumulo processes using the &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;-a&lt;/code&gt; argument and set the
hostname to the IP address. Solving connectivity issues between components
running in Kubernetes and components external to Kubernetes depends on the capabilities
available in the environment and the &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;-a&lt;/code&gt; option may be part of the solution.&lt;/p&gt;

&lt;h2 id=&quot;conclusion&quot;&gt;Conclusion&lt;/h2&gt;

&lt;p&gt;In this blog post we introduced the concept and benefits of external
compactions, the new server processes and how to configure the compaction
service. We deployed a 23-node Accumulo cluster using Muchos with a variable
sized Kubernetes cluster that dynamically scaled Compactors on 3 to 100 compute
nodes from 10 to 660 instances. We ran continuous ingest on the Accumulo
cluster to create compactions that were run both internal and external to the
Tablet Server and demonstrated external compactions completing successfully and
Compactors being killed.&lt;/p&gt;

&lt;p&gt;We discussed also running the following test, but did not have time.&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;Agitating the Compaction Coordinator, Tablet Servers and Compactors while ingest was running.&lt;/li&gt;
  &lt;li&gt;Comparing the impact on queries for internal vs external compactions.&lt;/li&gt;
  &lt;li&gt;Having multiple external compaction queues, each with its own set of autoscaled Compactor pods.&lt;/li&gt;
  &lt;li&gt;Forcing full table compactions while ingest was running.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;The test we ran shows that basic functionality works well, it would be nice to
stress the feature in other ways though.&lt;/p&gt;

</description>
        <pubDate>Thu, 08 Jul 2021 00:00:00 +0000</pubDate>
        <link>https://accumulo.apache.org/blog/2021/07/08/external-compactions.html</link>
        <guid isPermaLink="true">https://accumulo.apache.org/blog/2021/07/08/external-compactions.html</guid>
        
        
        <category>blog</category>
        
      </item>
    
      <item>
        <title>Jshell Accumulo Feature</title>
        <description>&lt;h2 id=&quot;overview&quot;&gt;Overview&lt;/h2&gt;

&lt;p&gt;First introduced in Java 9, &lt;a href=&quot;https://docs.oracle.com/javase/9/jshell/introduction-jshell.htm#JSHEL-GUID-630F27C8-1195-4989-9F6B-2C51D46F52C8&quot;&gt;JShell&lt;/a&gt; is an interactive Read-Evaluate-Print-Loop (REPL) 
Java tool that interprets user’s input and outputs the results. This tool provides a convenient 
way to test out and execute quick tasks with Accumulo in the terminal. This feature is a part 
of the upcoming Accumulo 2.1 release. If you’re a developer and want to get involved in testing, 
&lt;a href=&quot;https://accumulo.apache.org/contact-us/&quot;&gt;contact us&lt;/a&gt; or review our &lt;a href=&quot;https://accumulo.apache.org/how-to-contribute/&quot;&gt;contributing guide&lt;/a&gt;.&lt;/p&gt;

&lt;h2 id=&quot;major-features&quot;&gt;Major Features&lt;/h2&gt;
&lt;ul&gt;
  &lt;li&gt;
    &lt;p&gt;Default JShell script provides initial imports for interacting with Accumulo’s API and 
provided in Accumulo’s binary distribution tarball&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;On startup, JShell Accumulo  will automatically import the &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;CLASSPATH&lt;/code&gt;, load in a configured 
environment from user’s &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;conf/accumulo-env.sh&lt;/code&gt;, and invoke &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;conf/jshell-init.jsh&lt;/code&gt; 
to allow rapid Accumulo task executions&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;JShell Accumulo can startup using default/custom JShell script and users can append any JShell 
command-line &lt;a href=&quot;https://docs.oracle.com/javase/9/tools/jshell.htm#JSWOR-GUID-C337353B-074A-431C-993F-60C226163F00&quot;&gt;options&lt;/a&gt; to the startup command&lt;/p&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;booting-up-jshell-accumulo&quot;&gt;Booting Up JShell Accumulo&lt;/h2&gt;
&lt;p&gt;1) Open up a terminal and navigate to Accumulo’s installation directory&lt;/p&gt;

&lt;p&gt;2) To startup JShell with &lt;strong&gt;default script&lt;/strong&gt; use this command:&lt;/p&gt;

&lt;div class=&quot;language-bash highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;nv&quot;&gt;$ &lt;/span&gt;bin/accumulo jshell 
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;
&lt;p&gt;3) To startup JShell with &lt;strong&gt;custom script&lt;/strong&gt; use this command:&lt;/p&gt;

&lt;div class=&quot;language-bash highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;nv&quot;&gt;$ &lt;/span&gt;bin/accumulo jshell &lt;span class=&quot;nt&quot;&gt;--startup&lt;/span&gt; file/path/to/custom_script.jsh
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;
&lt;p&gt;&lt;strong&gt;Note:&lt;/strong&gt; One can execute the &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;jshell&lt;/code&gt; command to startup JShell. However, doing so will require 
manually importing the &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;CLASSPATH&lt;/code&gt; and the configured environment from &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;conf/accumulo-env.sh&lt;/code&gt; 
and manually specifying the startup file for &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;conf/jshell-init.jsh&lt;/code&gt; before any Accumulo tasks 
can be performed. Using one of the startup commands above will automate that process 
for convenience.&lt;/p&gt;

&lt;h2 id=&quot;jshell-accumulo-default-script&quot;&gt;JShell Accumulo Default Script&lt;/h2&gt;
&lt;p&gt;The auto-generated &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;jshell-init.jsh&lt;/code&gt; is a customizable file located in Accumulo’s installation 
&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;conf/&lt;/code&gt; directory. Inside, &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;jshell-init.jsh&lt;/code&gt; contains &lt;a href=&quot;https://accumulo.apache.org/api/&quot;&gt;Accumulo Java APIs&lt;/a&gt; 
formatted as import statements and &lt;a href=&quot;https://www.javadoc.io/doc/org.apache.accumulo/accumulo-core/latest/org/apache/accumulo/core/client/AccumuloClient.html&quot;&gt;AccumuloClient&lt;/a&gt; build implementation. On startup, 
the script automatically loads in the APIs and attempts to construct a client. Should additional 
APIs and/or code implementations be needed, simply append them to &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;jshell-init.jsh&lt;/code&gt;. 
Alternatively, you can create a separate JShell script and specify the custom script’s file path 
on startup.&lt;/p&gt;

&lt;p&gt;To construct an &lt;a href=&quot;https://www.javadoc.io/doc/org.apache.accumulo/accumulo-core/latest/org/apache/accumulo/core/client/AccumuloClient.html&quot;&gt;AccumuloClient&lt;/a&gt;, the provided &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;conf/jshell-init.jsh&lt;/code&gt; script finds 
and uses &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;accumulo-client.properties&lt;/code&gt; in Accumulo’s class path, and assigns the result 
to a variable called &lt;strong&gt;client&lt;/strong&gt;.&lt;/p&gt;

&lt;p&gt;If &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;accumulo-client.properties&lt;/code&gt; is found, a similar result will be produced below:&lt;/p&gt;

&lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;Preparing JShell for Apache Accumulo 

Building Accumulo client using 'jar:file:/home/accumulo/lib/accumulo-client.jar!/accumulo-client.properties'

Use 'client' to interact with Accumulo

|  Welcome to JShell -- Version 11.0.10
|  For an introduction type: /help intro

jshell&amp;gt; 
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;If &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;accumulo-client.properties&lt;/code&gt; is not found, an &lt;a href=&quot;https://www.javadoc.io/doc/org.apache.accumulo/accumulo-core/latest/org/apache/accumulo/core/client/AccumuloClient.html&quot;&gt;AccumuloClient&lt;/a&gt; will not 
auto-generate and will produce the following result below:&lt;/p&gt;

&lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;Preparing JShell for Apache Accumulo 

'accumulo-client.properties' was not found on the classpath

|  Welcome to JShell -- Version 11.0.10
|  For an introduction type: /help intro

jshell&amp;gt; 
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;h2 id=&quot;jshell-accumulo-example&quot;&gt;JShell Accumulo Example&lt;/h2&gt;
&lt;p&gt;1) Booting up JShell Accumulo using default script&lt;/p&gt;

&lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;Preparing JShell for Apache Accumulo 

Building Accumulo client using 'file:/home/accumulo/conf/accumulo-client.properties'

Use 'client' to interact with Accumulo

|  Welcome to JShell -- Version 11.0.10
|  For an introduction type: /help intro

jshell&amp;gt; 
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;2) Providing JShell with an Accumulo task&lt;/p&gt;

&lt;pre&gt;&lt;code class=&quot;language-Java&quot;&gt;  // Create a table called &quot;GothamPD&quot;.
  client.tableOperations().create(&quot;GothamPD&quot;);

  // Create a Mutation object to hold all changes to a row in a table. 
  // Each row has a unique row ID.
  Mutation mutation = new Mutation(&quot;id0001&quot;);

  // Create key/value pairs for Batman. Put them in the &quot;hero&quot; family.
  mutation.put(&quot;hero&quot;, &quot;alias&quot;, &quot;Batman&quot;);
  mutation.put(&quot;hero&quot;, &quot;name&quot;, &quot;Bruce Wayne&quot;);
  mutation.put(&quot;hero&quot;, &quot;wearsCape?&quot;, &quot;true&quot;);

  // Create a BatchWriter to the GothamPD table and add your mutation to it. 
  // Try w/ resources will close for us.
  try (BatchWriter writer = client.createBatchWriter(&quot;GothamPD&quot;)) {
      writer.addMutation(mutation);
  }
  
  // Read and print all rows of the &quot;GothamPD&quot; table. 
  // Try w/ resources will close for us.
  try (ScannerBase scan = client.createScanner(&quot;GothamPD&quot;, Authorizations.EMPTY)) {
    System.out.println(&quot;Gotham Police Department Persons of Interest:&quot;);
    
    // A Scanner is an extension of java.lang.Iterable so behaves just like one.
    scan.forEach((k, v) -&amp;gt; System.out.printf(&quot;Key : %-50s Value : %s\n&quot;, k, v));
  }
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;&lt;strong&gt;Note:&lt;/strong&gt; The fully-qualified class name for Accumulo Scanner or 
&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;org.apache.accumulo.core.client.Scanner&lt;/code&gt; needs to be used due to conflicting issues with 
Java’s built-in java.util.Scanner. However, to shorten the Accumulo Scanner’s declaration, assign 
scan to &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;ScannerBase&lt;/code&gt; type instead.&lt;/p&gt;

&lt;p&gt;3) Executing the Accumulo task above outputs:&lt;/p&gt;

&lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;mutation ==&amp;gt; org.apache.accumulo.core.data.Mutation@1
Gotham Police Department Persons of Interest:
Key : id0001 hero:alias [] 1618926204602 false            Value : Batman
Key : id0001 hero:name [] 1618926204602 false             Value : Bruce Wayne
Key : id0001 hero:wearsCape? [] 1618926204602 false       Value : true

jshell&amp;gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;
</description>
        <pubDate>Wed, 21 Apr 2021 00:00:00 +0000</pubDate>
        <link>https://accumulo.apache.org/blog/2021/04/21/jshell-accumulo-feature.html</link>
        <guid isPermaLink="true">https://accumulo.apache.org/blog/2021/04/21/jshell-accumulo-feature.html</guid>
        
        
        <category>blog</category>
        
      </item>
    
      <item>
        <title>Apache Accumulo 2.0.1</title>
        <description>&lt;p&gt;Apache Accumulo 2.0.1 contains bug fixes for 2.0.0.&lt;/p&gt;

&lt;p&gt;Since 2.0 is a non-LTM release line, and since an LTM release line has not yet
been made available for 2.x, this patch backports critical bug fixes to 2.0 to
address security bug &lt;a href=&quot;https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2020-17533&quot;&gt;CVE-2020-17533&lt;/a&gt; that could affect any existing 2.0.0
users. Users that have already migrated to 2.0.0 are urged to upgrade to 2.0.1
as soon as possible, and users of 1.10 who wish to upgrade to 2.0 should
upgrade directly to 2.0.1, bypassing 2.0.0.&lt;/p&gt;

&lt;p&gt;These release notes are highlights of the changes since 2.0.0. The full
detailed changes can be seen in the git history. If anything is missing from
this list, please &lt;a href=&quot;/contact-us&quot;&gt;contact&lt;/a&gt; us to have it included.&lt;/p&gt;

&lt;h2 id=&quot;critical-bug-fixes&quot;&gt;Critical Bug Fixes&lt;/h2&gt;

&lt;p&gt;This release includes critical bug fixes to fix security bugs identified as
&lt;a href=&quot;https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2020-17533&quot;&gt;CVE-2020-17533&lt;/a&gt;:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;&lt;a href=&quot;https://github.com/apache/accumulo/issues/1828&quot;&gt;#1828&lt;/a&gt;, &lt;a href=&quot;https://github.com/apache/accumulo/issues/1832&quot;&gt;#1832&lt;/a&gt; Throw exceptions when permission checks fail,
and improve test coverage for permissions checks&lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&quot;other-bug-fixes&quot;&gt;Other Bug Fixes&lt;/h3&gt;

&lt;ul&gt;
  &lt;li&gt;&lt;a href=&quot;https://github.com/apache/accumulo/issues/1839&quot;&gt;#1839&lt;/a&gt; Fix AccumuloClient’s builder to prevent it from modifying a
provided Properties object when building a client from Properties&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;note-about-newer-jdk-versions-11-and-later&quot;&gt;Note About Newer JDK Versions (11 and later)&lt;/h2&gt;

&lt;p&gt;While work has been done on other branches to better support newer JDK
versions, that is not the case for this non-LTM release. Certain non-critical
aspects of this release are known to break with some newer versions of JDK.&lt;/p&gt;

&lt;p&gt;For example, the version of &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;maven-javadoc-plugin&lt;/code&gt; may fail to generate the
javadocs using a newer JDK’s javadoc tool. In addition, this release assumes
the use of the CMS garbage collector in its build tests and in minicluster.
Newer JDKs, where CMS has been removed, may fail to execute Accumulo build
tests in this release.&lt;/p&gt;

&lt;p&gt;Therefore, it is recommended to use JDK 8 or JDK 11 with this release, which
are known to work.&lt;/p&gt;

&lt;h2 id=&quot;note-about-zookeeper-versions-35-and-later&quot;&gt;Note About ZooKeeper Versions 3.5 and Later&lt;/h2&gt;

&lt;p&gt;This release assumes the use of ZooKeeper 3.4. While work has been done on
other branches to better support newer ZooKeeper versions (3.5 and later), this
is a targeted release to fix specific bugs and does not include those kinds of
improvements.&lt;/p&gt;

&lt;p&gt;Therefore, in order to use this release with ZooKeeper versions 3.5 and later,
you may need to edit your default class path, or perform other minor changes to
work smoothly with those versions of ZooKeeper. Please &lt;a href=&quot;/contact-us&quot;&gt;contact&lt;/a&gt; us if you need
assistance working with newer versions of ZooKeeper.&lt;/p&gt;

&lt;h2 id=&quot;upgrading&quot;&gt;Upgrading&lt;/h2&gt;

&lt;p&gt;View the &lt;a href=&quot;/docs/2.x/administration/upgrading&quot;&gt;Upgrading Accumulo documentation&lt;/a&gt; for guidance.&lt;/p&gt;

&lt;h2 id=&quot;useful-links&quot;&gt;Useful Links&lt;/h2&gt;

&lt;ul&gt;
  &lt;li&gt;&lt;a href=&quot;https://lists.apache.org/thread.html/r0835b67240060cae394c7e4a7ad18a7238f17cabc7a508aa176c95c9%40%3Cdev.accumulo.apache.org%3E&quot;&gt;Release VOTE email thread&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;https://github.com/apache/accumulo/compare/rel/2.0.0...apache:rel/2.0.1&quot;&gt;All Changes since 2.0.0&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;https://github.com/apache/accumulo/issues?q=project%3Aapache%2Faccumulo%2F19&quot;&gt;GitHub&lt;/a&gt; - List of issues tracked on GitHub corresponding to this release&lt;/li&gt;
&lt;/ul&gt;

</description>
        <pubDate>Thu, 24 Dec 2020 00:00:00 +0000</pubDate>
        <link>https://accumulo.apache.org/release/accumulo-2.0.1/</link>
        <guid isPermaLink="true">https://accumulo.apache.org/release/accumulo-2.0.1/</guid>
        
        
        <category>release</category>
        
      </item>
    
      <item>
        <title>Apache Accumulo 1.10.1</title>
        <description>&lt;h2 id=&quot;about&quot;&gt;About&lt;/h2&gt;

&lt;p&gt;Apache Accumulo 1.10.1 is a bug fix release of the 1.10 LTM release line.&lt;/p&gt;

&lt;p&gt;These release notes are highlights of the changes since 1.10.0. The full
detailed changes can be seen in the git history. If anything is missing from
this list, please &lt;a href=&quot;/contact-us&quot;&gt;contact&lt;/a&gt; us to have it included.&lt;/p&gt;

&lt;p&gt;Users of 1.10.0 or earlier are urged to upgrade to 1.10.1 as soon as possible,
as this is a continuation of the 1.10 LTM release line with critical bug fixes
for security bug &lt;a href=&quot;https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2020-17533&quot;&gt;CVE-2020-17533&lt;/a&gt;. Users are also encouraged to consider
migrating to a 2.x version when one that is suitable for their needs becomes
available.&lt;/p&gt;

&lt;h2 id=&quot;critical-bug-fixes&quot;&gt;Critical Bug Fixes&lt;/h2&gt;

&lt;p&gt;This release includes critical bug fixes to fix security bugs identified as
&lt;a href=&quot;https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2020-17533&quot;&gt;CVE-2020-17533&lt;/a&gt;:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;&lt;a href=&quot;https://github.com/apache/accumulo/issues/1830&quot;&gt;#1830&lt;/a&gt;, &lt;a href=&quot;https://github.com/apache/accumulo/issues/1832&quot;&gt;#1832&lt;/a&gt; Throw exceptions when permission checks fail,
and improve test coverage for permissions checks (backport of &lt;a href=&quot;https://github.com/apache/accumulo/issues/1828&quot;&gt;#1828&lt;/a&gt;)&lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&quot;other-bug-fixes&quot;&gt;Other Bug Fixes&lt;/h3&gt;

&lt;ul&gt;
  &lt;li&gt;&lt;a href=&quot;https://github.com/apache/accumulo/issues/1716&quot;&gt;#1716&lt;/a&gt;, &lt;a href=&quot;https://github.com/apache/accumulo/issues/1729&quot;&gt;#1729&lt;/a&gt;, &lt;a href=&quot;https://github.com/apache/accumulo/issues/1737&quot;&gt;#1737&lt;/a&gt; Improvements in tool.sh,
including better support for newer ZooKeeper and Hadoop versions&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;https://github.com/apache/accumulo/issues/1829&quot;&gt;#1829&lt;/a&gt; Improve log message in Delete Cleanup FATE&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;https://github.com/apache/accumulo/issues/1734&quot;&gt;#1734&lt;/a&gt; Support building native libraries on alpine-based distros&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;note-about-jdk-15&quot;&gt;Note About JDK 15&lt;/h2&gt;

&lt;p&gt;Accumulo 1.x assumes the use of the CMS garbage collector in its build tests
and in the minicluster code. That garbage collector was removed in newer
versions of Java, and the build flags for Java that supported configuring the
CMS garbage collector now cause errors if attempted to be used with Java 15 or
later.&lt;/p&gt;

&lt;p&gt;Therefore, a change was made in 1.10.1’s build to fail fast if attempting to
build with JDK 15 or later (using JDK 11 or later was already a build
requirement).&lt;/p&gt;

&lt;p&gt;If you need to build on JDK 15 or later, and intend to skip tests and don’t
intend to use minicluster, you can bypass this build constraint by building
with &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;-Denforcer.skip&lt;/code&gt;, as a workaround.&lt;/p&gt;

&lt;h2 id=&quot;useful-links&quot;&gt;Useful Links&lt;/h2&gt;

&lt;ul&gt;
  &lt;li&gt;&lt;a href=&quot;https://lists.apache.org/thread.html/r90ac3cc0d2356c86a94abf2b6859965e9659b8bcdb6cfd18b69941ac%40%3Cdev.accumulo.apache.org%3E&quot;&gt;Release VOTE email thread&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;https://github.com/apache/accumulo/compare/rel/1.10.0...apache:rel/1.10.1&quot;&gt;All Changes since 1.10.0&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;https://github.com/apache/accumulo/issues?q=project%3Aapache%2Faccumulo%2F16&quot;&gt;GitHub&lt;/a&gt; - List of issues tracked on GitHub corresponding to this release&lt;/li&gt;
&lt;/ul&gt;

</description>
        <pubDate>Tue, 22 Dec 2020 00:00:00 +0000</pubDate>
        <link>https://accumulo.apache.org/release/accumulo-1.10.1/</link>
        <guid isPermaLink="true">https://accumulo.apache.org/release/accumulo-1.10.1/</guid>
        
        
        <category>release</category>
        
      </item>
    
      <item>
        <title>Apache Accumulo 1.10.0</title>
        <description>&lt;h2 id=&quot;about&quot;&gt;About&lt;/h2&gt;

&lt;p&gt;Apache Accumulo 1.10.0 is a continuation of the 1.x release line, and is
essentially the next maintenance release of 1.8/1.9, following the 1.9.3
version with some small additional internal improvements. Earlier 1.x versions
are now superseded by this maintenance release, and will no longer be
maintained.&lt;/p&gt;

&lt;p&gt;The &lt;a href=&quot;https://semver.org/spec/v2.0.0.html&quot;&gt;semver&lt;/a&gt; minor version number increase (1.9 to 1.10) signals that this
release is backwards compatible with previous minor releases (1.8 and 1.9).
Rather than API additions, the primary reason for this minor version increase
is due to the decision to make Java 8 the minimum supported Java version (see
below for more).&lt;/p&gt;

&lt;p&gt;This release contains contributions from more than 13 contributors from the
Apace Accumulo community in over 80 commits and 16 months of work since the
1.9.3 release. These release notes are highlights of those changes. The full
detailed changes can be seen in the git history. If anything is missing from
this list, please &lt;a href=&quot;/contact-us&quot;&gt;contact&lt;/a&gt; us to have it included.&lt;/p&gt;

&lt;p&gt;According to the &lt;a href=&quot;/contributor/versioning#LTM&quot;&gt;Long Term Maintenance (LTM)&lt;/a&gt; strategy, the intent is to
maintain the 1.10 release line with critical bug and security fixes until one
year after the next LTM version is released. However, this is anticipated to be
the final 1.x legacy release, so it is not expected to receive any new features
or significant non-critical updates, so users wanting new features should plan
to upgrade to a 2.x release, where new feature development is still being done.&lt;/p&gt;

&lt;p&gt;Users of 1.9.3 or earlier are urged to upgrade to 1.10.0 as soon as it is
available, as this is a continuation of the 1.9 maintenance line. and to
consider migrating to a 2.x version when a suitable one becomes available.
Accumulo 2.0.0 is currently available, and 2.1.0 is anticipated to be the next
LTM release. If you would like to start preparing for 2.1.0 now, one way to do
this is to start building and testing the next version of your software against
Accumulo 2.0.0 because all 2.x releases should be backwards compatible with
2.0.0, following &lt;a href=&quot;https://semver.org/spec/v2.0.0.html&quot;&gt;semantic versioning&lt;/a&gt;.&lt;/p&gt;

&lt;h2 id=&quot;minimum-requirements&quot;&gt;Minimum Requirements&lt;/h2&gt;

&lt;p&gt;The versions mentioned here are a guide only. It is not expected that our
convenience binary tarball will work out-of-the-box with your particular
environment, and some responsibility is placed on users to properly configure
Accumulo, or even patch and rebuild it from source, for their particular
environment.&lt;/p&gt;

&lt;p&gt;Please &lt;a href=&quot;/contact-us&quot;&gt;contact&lt;/a&gt; us or file a &lt;a href=&quot;https://github.com/apache/accumulo/issues&quot;&gt;bug report&lt;/a&gt; if you have trouble with a
specific version or wish to seek tips. Be prepared to provide details of the
problems you encounter, as well as perform some troubleshooting steps of your
own, in order to get the best response.&lt;/p&gt;

&lt;h3 id=&quot;java-8&quot;&gt;Java 8&lt;/h3&gt;

&lt;p&gt;Java 8 is now the minimum supported Java version, and it is designed to work on
Java 11, as well. To build the project from source, Java 11 or later is
required. Please &lt;a href=&quot;/contact-us&quot;&gt;contact&lt;/a&gt; us if you find any bugs on any Java version.&lt;/p&gt;

&lt;h3 id=&quot;hadoop-2-or-3&quot;&gt;Hadoop 2 or 3&lt;/h3&gt;

&lt;p&gt;This release has been built using Hadoop 2.6.5, and is expected to work with
any Hadoop version 2.6.5 or later. It has also been tested with 3.0.3, and is
expected to work with Hadoop 3.0 versions as well. Hadoop 3.1.3, 3.2.1, and
3.3.0 have also been tested with this version, and are known to work (with at
least basic functionality) with some class path modifications (specifically,
using Guava 27.0-jre instead of the provided 14.0 version).&lt;/p&gt;

&lt;p&gt;Particular class path pain points are known to be guava, commons-io,
commons-vfs2, and possibly other commons libraries.&lt;/p&gt;

&lt;h3 id=&quot;zookeeper&quot;&gt;ZooKeeper&lt;/h3&gt;

&lt;p&gt;This release has been built agains ZooKeeper 3.4.14, the latest 3.4 release. It
is known to work against 3.5 and 3.6 versions as well, when configured
properly.&lt;/p&gt;

&lt;h2 id=&quot;major-bug-fixes&quot;&gt;Major Bug Fixes&lt;/h2&gt;

&lt;h3 id=&quot;accumulo-gc-bug&quot;&gt;Accumulo GC Bug&lt;/h3&gt;

&lt;ul&gt;
  &lt;li&gt;&lt;a href=&quot;https://github.com/apache/accumulo/issues/1314&quot;&gt;#1314&lt;/a&gt;, &lt;a href=&quot;https://github.com/apache/accumulo/issues/1318&quot;&gt;#1318&lt;/a&gt; Eliminate task creation leak caused by the an
additional timed-task created for each &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;accumulo-gc&lt;/code&gt; cycle&lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&quot;bulk-import-concurrency-bug&quot;&gt;Bulk Import Concurrency Bug&lt;/h3&gt;

&lt;ul&gt;
  &lt;li&gt;&lt;a href=&quot;https://github.com/apache/accumulo/issues/1153&quot;&gt;#1153&lt;/a&gt; Prevent multiple threads from working on same bulk file&lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&quot;prevent-metadata-corruption&quot;&gt;Prevent Metadata Corruption&lt;/h3&gt;

&lt;ul&gt;
  &lt;li&gt;&lt;a href=&quot;https://github.com/apache/accumulo/issues/1309&quot;&gt;#1309&lt;/a&gt; Prevent cloning of the metadata table, which could lead to
data loss during &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;accumulo-gc&lt;/code&gt; for either the clone or the original
&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;accumulo.metadata&lt;/code&gt; table&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;https://github.com/apache/accumulo/issues/1310&quot;&gt;#1310&lt;/a&gt; Improve GC handling of WALs used by root tablet. If the root
tablet had WALs, the GC did not consider them during collection&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;https://github.com/apache/accumulo/issues/1379&quot;&gt;#1379&lt;/a&gt; During GC scans, an error will be thrown if the GC fails
consistency checks; added a check to ensure the last tablet was seen&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;other-miscellaneous-bug-fixes&quot;&gt;Other Miscellaneous Bug Fixes&lt;/h2&gt;

&lt;ul&gt;
  &lt;li&gt;&lt;a href=&quot;https://github.com/apache/accumulo/issues/1107&quot;&gt;#1107&lt;/a&gt; Fix &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;ConcurrentModificationException&lt;/code&gt; in
&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;HostRegexTableLoadBalancer&lt;/code&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;https://github.com/apache/accumulo/issues/1185&quot;&gt;#1185&lt;/a&gt; Fixed a bug where we were using an unauthenticated ZooKeeper
client to try to read data with an ACL configured; this was previously
permitted until ZooKeeper fixed a security bug in their own code, which
revealed our incorrect ZooKeeper client code&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;https://github.com/apache/accumulo/issues/1371&quot;&gt;#1371&lt;/a&gt; Fix a bug in our MapReduce code that prevented some users from
reading tables they had valid permissions to read&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;https://github.com/apache/accumulo/issues/1401&quot;&gt;#1401&lt;/a&gt; Display trace information correctly in &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;accumulo-monitor&lt;/code&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;https://github.com/apache/accumulo/issues/1478&quot;&gt;#1478&lt;/a&gt; Don’t ignore the instance and zookeepers parameters on the
command-line when running certain utilities&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;https://github.com/apache/accumulo/issues/1532&quot;&gt;#1532&lt;/a&gt; Remove need for ANT on classpath&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;https://github.com/apache/accumulo/issues/1555&quot;&gt;#1555&lt;/a&gt; Fix idempotency bug in importtable&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;https://github.com/apache/accumulo/issues/1644&quot;&gt;#1644&lt;/a&gt; Retry minor compactions to prevent transient iterator issues
blocking forever&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;major-improvements&quot;&gt;Major Improvements&lt;/h2&gt;

&lt;h3 id=&quot;performance-enhancements&quot;&gt;Performance Enhancements&lt;/h3&gt;

&lt;ul&gt;
  &lt;li&gt;&lt;a href=&quot;https://github.com/apache/accumulo/issues/990&quot;&gt;#990&lt;/a&gt; Avoid multiple threads loading same cache block&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;https://github.com/apache/accumulo/issues/1352&quot;&gt;#1352&lt;/a&gt; Add an option to configure the metadata action after an
&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;accumulo-gc&lt;/code&gt; cycle using a new property instead of a hard-coded compaction
    &lt;ul&gt;
      &lt;li&gt;&lt;a href=&quot;/1.10/accumulo_user_manual#_gc_post_metadata_action&quot;&gt;gc.post.metadata.action&lt;/a&gt;&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;https://github.com/apache/accumulo/issues/1462&quot;&gt;#1462&lt;/a&gt;, &lt;a href=&quot;https://github.com/apache/accumulo/issues/1526&quot;&gt;#1526&lt;/a&gt; Temporarily cache the existence check for
recovery WALs, so multiple tablets pointing to the same WAL to avoid
expensive redundant checks&lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&quot;identifying-busy-tablets&quot;&gt;Identifying Busy Tablets&lt;/h3&gt;

&lt;ul&gt;
  &lt;li&gt;&lt;a href=&quot;https://github.com/apache/accumulo/issues/1291&quot;&gt;#1291&lt;/a&gt;, &lt;a href=&quot;https://github.com/apache/accumulo/issues/1296&quot;&gt;#1296&lt;/a&gt; Log busy tablets by ingest and query at
configurable intervals for better hot-spot detection using new properties
    &lt;ul&gt;
      &lt;li&gt;&lt;a href=&quot;/1.10/accumulo_user_manual#_tserver_log_busy_tablets_count&quot;&gt;tserver.log.busy.tablets.count&lt;/a&gt; and&lt;/li&gt;
      &lt;li&gt;&lt;a href=&quot;/1.10/accumulo_user_manual#_tserver_log_busy_tablets_interval&quot;&gt;tserver.log.busy.tablets.interval&lt;/a&gt;&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&quot;tserver-startup-and-shutdown-protections&quot;&gt;TServer Startup and Shutdown Protections&lt;/h3&gt;

&lt;ul&gt;
  &lt;li&gt;&lt;a href=&quot;https://github.com/apache/accumulo/issues/1158&quot;&gt;#1158&lt;/a&gt; Require a configurable number of servers to be online, up to a
max wait time, before assignments begin on startup
    &lt;ul&gt;
      &lt;li&gt;&lt;a href=&quot;/1.10/accumulo_user_manual#_master_startup_tserver_avail_min_count&quot;&gt;master.startup.tserver.avail.min.count&lt;/a&gt;&lt;/li&gt;
      &lt;li&gt;&lt;a href=&quot;/1.10/accumulo_user_manual#_master_startup_tserver_avail_max_wait&quot;&gt;master.startup.tserver.avail.max.wait&lt;/a&gt;&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;https://github.com/apache/accumulo/issues/1456&quot;&gt;#1456&lt;/a&gt; Throttle the number of shutdown requests sent to tservers to
prevent cluster self-destruction and give time for triage&lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&quot;new-metrics&quot;&gt;New Metrics&lt;/h3&gt;

&lt;ul&gt;
  &lt;li&gt;&lt;a href=&quot;https://github.com/apache/accumulo/issues/1406&quot;&gt;#1406&lt;/a&gt; Add GC cycle metrics (file and wal collection) to be reported
via the hadoop2 metrics. This exposes the gc cycle metrics available in the
monitor to external metrics systems and includes run time for the new gc post
operation (compact, flush)
    &lt;ul&gt;
      &lt;li&gt;Enable with new property, &lt;a href=&quot;/1.10/accumulo_user_manual#_gc_metrics_enabled&quot;&gt;gc.metrics.enabled&lt;/a&gt;
        &lt;ul&gt;
          &lt;li&gt;&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;AccGcCandidates&lt;/code&gt; - number of candidates for GC&lt;/li&gt;
          &lt;li&gt;&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;AccGcDeleted&lt;/code&gt; - number of candidates deleted&lt;/li&gt;
          &lt;li&gt;&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;AccGcErrors&lt;/code&gt; - number of deletion errors&lt;/li&gt;
          &lt;li&gt;&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;AccGcFinished&lt;/code&gt; - timestamp of GC cycle finished&lt;/li&gt;
          &lt;li&gt;&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;AccGcInUse&lt;/code&gt; - number of candidates still in use&lt;/li&gt;
          &lt;li&gt;&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;AccGcPostOpDuration&lt;/code&gt; - duration of compact / flush&lt;/li&gt;
          &lt;li&gt;&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;AccGcRunCycleCount&lt;/code&gt; - 1-up cycle count&lt;/li&gt;
          &lt;li&gt;&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;AccGcStarted&lt;/code&gt; - timestamp of GC cycle start&lt;/li&gt;
          &lt;li&gt;&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;AccGcWalCandidates&lt;/code&gt; - number of WAL candidates for collection&lt;/li&gt;
          &lt;li&gt;&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;AccGcWalDeleted&lt;/code&gt; - number of WALs deleted&lt;/li&gt;
          &lt;li&gt;&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;AccGcWalErrors&lt;/code&gt; - number of errors during WAL deletion&lt;/li&gt;
          &lt;li&gt;&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;AccGcWalFinished&lt;/code&gt; - timestamp of WAL collection completion&lt;/li&gt;
          &lt;li&gt;&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;AccGcWalInUse&lt;/code&gt; - number of WALs in use&lt;/li&gt;
          &lt;li&gt;&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;AccGcWalStarted&lt;/code&gt; - timestamp of WAL collection start&lt;/li&gt;
        &lt;/ul&gt;
      &lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;other-miscellaneous-improvements&quot;&gt;Other Miscellaneous Improvements&lt;/h2&gt;

&lt;ul&gt;
  &lt;li&gt;&lt;a href=&quot;https://github.com/apache/accumulo/issues/1108&quot;&gt;#1108&lt;/a&gt; Improve logging when ZooKeeper session expires&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;https://github.com/apache/accumulo/issues/1299&quot;&gt;#1299&lt;/a&gt; Add optional &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;-t&lt;/code&gt; tablename to importdirectory shell command&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;https://github.com/apache/accumulo/issues/1338&quot;&gt;#1338&lt;/a&gt; Reduce verbose logging of merge operations in Master log&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;https://github.com/apache/accumulo/issues/1475&quot;&gt;#1475&lt;/a&gt; Option to leave cloned tables offline on creation&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;https://github.com/apache/accumulo/issues/1503&quot;&gt;#1503&lt;/a&gt; Support ZooKeeper 3.5 (and later), in addition to 3.4&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;useful-links&quot;&gt;Useful Links&lt;/h2&gt;

&lt;ul&gt;
  &lt;li&gt;&lt;a href=&quot;https://lists.apache.org/thread.html/rd4731d4fd87c30958ad82a8b0be9375f2562ab0a9531ea037e646f3c%40%3Cdev.accumulo.apache.org%3E&quot;&gt;Release VOTE email thread&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;https://github.com/apache/accumulo/compare/rel/1.9.3...apache:rel/1.10.0&quot;&gt;All Changes since 1.9.3&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;https://github.com/apache/accumulo/issues?q=project%3Aapache%2Faccumulo%2F8&quot;&gt;GitHub&lt;/a&gt; - List of issues tracked on GitHub corresponding to this release&lt;/li&gt;
&lt;/ul&gt;

</description>
        <pubDate>Thu, 03 Sep 2020 00:00:00 +0000</pubDate>
        <link>https://accumulo.apache.org/release/accumulo-1.10.0/</link>
        <guid isPermaLink="true">https://accumulo.apache.org/release/accumulo-1.10.0/</guid>
        
        
        <category>release</category>
        
      </item>
    
      <item>
        <title>Microsoft MASC, an Apache Spark connector for Apache Accumulo</title>
        <description>&lt;h1 id=&quot;overview&quot;&gt;Overview&lt;/h1&gt;
&lt;p&gt;&lt;a href=&quot;https://github.com/microsoft/masc&quot;&gt;MASC&lt;/a&gt; provides an Apache Spark native connector for Apache Accumulo to integrate the rich Spark machine learning eco-system with the scalable and secure data storage capabilities of Accumulo.&lt;/p&gt;

&lt;h2 id=&quot;major-features&quot;&gt;Major Features&lt;/h2&gt;
&lt;ul&gt;
  &lt;li&gt;Simplified Spark DataFrame read/write to Accumulo using DataSource v2 API&lt;/li&gt;
  &lt;li&gt;Speedup of 2-5x over existing approaches for pulling key-value data into DataFrame format&lt;/li&gt;
  &lt;li&gt;Scala and Python support without overhead for moving between languages&lt;/li&gt;
  &lt;li&gt;Process streaming data from Accumulo without loading it all into Spark memory&lt;/li&gt;
  &lt;li&gt;Push down filtering with a flexible expression language (&lt;a href=&quot;http://juel.sourceforge.net/&quot;&gt;JUEL&lt;/a&gt;): user can define logical operators and comparisons to reduce the amount of data returned from Accumulo&lt;/li&gt;
  &lt;li&gt;Column pruning based on selected fields transparently reduces the amount of data returned from Accumulo&lt;/li&gt;
  &lt;li&gt;Server side inference: ML model inference can run on the Accumulo nodes using MLeap to increase the scalability of AI solutions as well as keeping data in Accumulo&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;use-cases&quot;&gt;Use-cases&lt;/h2&gt;
&lt;p&gt;MASC is advantageous in many use-cases, below we list a few.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Scenario 1&lt;/strong&gt;: A data analyst needs to execute model inference on large amount of data in Accumulo.&lt;br /&gt;
&lt;strong&gt;Benefit&lt;/strong&gt;: Instead of transferring all the data to a large Spark cluster to score using a Spark model, the connector exports and runs the model on the Accumulo cluster. This reduces the need for a large Spark cluster as well as the amount of data transferred between systems, and can improve inference speeds (&amp;gt;2x speedups observed).&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Scenario 2&lt;/strong&gt;: A data scientist needs to train a Spark model on a large amount of data in Accumulo.&lt;br /&gt;
&lt;strong&gt;Benefit&lt;/strong&gt;: Instead of pulling all the data into a large Spark cluster and restructuring the format to use Spark ML Lib tools, the connector streams data into Spark as a DataFrame reducing time to train and Spark cluster size / memory requirements.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Scenario 3&lt;/strong&gt;: A data analyst needs to perform ad hoc analysis on large amounts of data stored in Accumulo.&lt;br /&gt;
&lt;strong&gt;Benefit&lt;/strong&gt;: Instead of pulling all the data into a large Spark cluster, the connector prunes rows and columns using pushdown filtering with a flexible expression language.&lt;/p&gt;

&lt;h1 id=&quot;architecture&quot;&gt;Architecture&lt;/h1&gt;
&lt;p&gt;The Accumulo-Spark connector is composed of two components:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;Accumulo server-side iterator performs
    &lt;ul&gt;
      &lt;li&gt;column pruning&lt;/li&gt;
      &lt;li&gt;row-based filtering&lt;/li&gt;
      &lt;li&gt;&lt;a href=&quot;https://github.com/combust/mleap&quot;&gt;MLeap&lt;/a&gt; ML model inference and&lt;/li&gt;
      &lt;li&gt;row assembly using &lt;a href=&quot;https://avro.apache.org/&quot;&gt;Apache AVRO&lt;/a&gt;&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;Spark DataSource V2
    &lt;ul&gt;
      &lt;li&gt;determines the number of Spark tasks based on available Accumulo table splits&lt;/li&gt;
      &lt;li&gt;translates Spark filter conditions into a &lt;a href=&quot;http://juel.sourceforge.net/&quot;&gt;JUEL&lt;/a&gt; expression&lt;/li&gt;
      &lt;li&gt;configures the Accumulo iterator&lt;/li&gt;
      &lt;li&gt;deserializes the AVRO payload&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;img src=&quot;/images/blog/202002_masc/architecture.svg&quot; alt=&quot;Architecture&quot; title=&quot;MASC Architecture Diagram&quot; /&gt;&lt;/p&gt;

&lt;h1 id=&quot;usage&quot;&gt;Usage&lt;/h1&gt;
&lt;p&gt;More detailed documentation on installation and use is available in the 
&lt;a href=&quot;https://github.com/microsoft/masc/blob/master/connector/README.md&quot;&gt;Connector documentation&lt;/a&gt;&lt;/p&gt;

&lt;h2 id=&quot;dependencies&quot;&gt;Dependencies&lt;/h2&gt;
&lt;ul&gt;
  &lt;li&gt;Java 8&lt;/li&gt;
  &lt;li&gt;Spark 2.4.3+&lt;/li&gt;
  &lt;li&gt;Accumulo 2.0.0+&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;JARs available on Maven Central Repository:&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;
    &lt;p&gt;&lt;a href=&quot;https://search.maven.org/search?q=g:%22com.microsoft.masc%22%20AND%20a:%22microsoft-accumulo-spark-datasource%22&quot;&gt;&lt;img src=&quot;https://img.shields.io/maven-central/v/com.microsoft.masc/microsoft-accumulo-spark-datasource.svg?label=Maven%20Central&quot; alt=&quot;Maven Central&quot; /&gt; Spark DataSource&lt;/a&gt;&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;&lt;a href=&quot;https://search.maven.org/search?q=g:%22com.microsoft.masc%22%20AND%20a:%22microsoft-accumulo-spark-iterator%22&quot;&gt;&lt;img src=&quot;https://img.shields.io/maven-central/v/com.microsoft.masc/microsoft-accumulo-spark-iterator.svg?label=Maven%20Central&quot; alt=&quot;Maven Central&quot; /&gt; Accumulo Iterator&lt;/a&gt; - Backend for Spark DataSource&lt;/p&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;example-use&quot;&gt;Example use&lt;/h2&gt;
&lt;div class=&quot;language-python highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;kn&quot;&gt;from&lt;/span&gt; &lt;span class=&quot;nn&quot;&gt;configparser&lt;/span&gt; &lt;span class=&quot;kn&quot;&gt;import&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;ConfigParser&lt;/span&gt;
&lt;span class=&quot;kn&quot;&gt;from&lt;/span&gt; &lt;span class=&quot;nn&quot;&gt;pyspark.sql&lt;/span&gt; &lt;span class=&quot;kn&quot;&gt;import&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;types&lt;/span&gt; &lt;span class=&quot;k&quot;&gt;as&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;T&lt;/span&gt;

&lt;span class=&quot;k&quot;&gt;def&lt;/span&gt; &lt;span class=&quot;nf&quot;&gt;get_properties&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;properties_file&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;):&lt;/span&gt;
    &lt;span class=&quot;s&quot;&gt;&quot;&quot;&quot;Read Accumulo client properties file&quot;&quot;&quot;&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;config&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;ConfigParser&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;()&lt;/span&gt;
    &lt;span class=&quot;k&quot;&gt;with&lt;/span&gt; &lt;span class=&quot;nb&quot;&gt;open&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;properties_file&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt; &lt;span class=&quot;k&quot;&gt;as&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;stream&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt;
        &lt;span class=&quot;n&quot;&gt;config&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;read_string&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&quot;[top]&lt;/span&gt;&lt;span class=&quot;se&quot;&gt;\n&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&quot;&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;+&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;stream&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;read&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;())&lt;/span&gt;
    &lt;span class=&quot;k&quot;&gt;return&lt;/span&gt; &lt;span class=&quot;nb&quot;&gt;dict&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;config&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;'top'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;])&lt;/span&gt;

&lt;span class=&quot;n&quot;&gt;properties&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;get_properties&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;'/opt/muchos/install/accumulo-2.0.0/conf/accumulo-client.properties'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;properties&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;'table'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;'demo_table'&lt;/span&gt; &lt;span class=&quot;c1&quot;&gt;# Define Accumulo table where data will be written
&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;properties&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;'rowkey'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;'id'&lt;/span&gt;        &lt;span class=&quot;c1&quot;&gt;# Identify column to use as the key for Accumulo rows
&lt;/span&gt;
&lt;span class=&quot;c1&quot;&gt;# define the schema
&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;schema&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;T&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;StructType&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;([&lt;/span&gt;
  &lt;span class=&quot;n&quot;&gt;T&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;StructField&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&quot;sentiment&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;T&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;IntegerType&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(),&lt;/span&gt; &lt;span class=&quot;bp&quot;&gt;True&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;),&lt;/span&gt;
  &lt;span class=&quot;n&quot;&gt;T&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;StructField&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&quot;date&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;T&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;StringType&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(),&lt;/span&gt; &lt;span class=&quot;bp&quot;&gt;True&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;),&lt;/span&gt;
  &lt;span class=&quot;n&quot;&gt;T&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;StructField&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&quot;query_string&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;T&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;StringType&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(),&lt;/span&gt; &lt;span class=&quot;bp&quot;&gt;True&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;),&lt;/span&gt;
  &lt;span class=&quot;n&quot;&gt;T&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;StructField&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&quot;user&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;T&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;StringType&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(),&lt;/span&gt; &lt;span class=&quot;bp&quot;&gt;True&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;),&lt;/span&gt;
  &lt;span class=&quot;n&quot;&gt;T&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;StructField&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&quot;text&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;T&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;StringType&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(),&lt;/span&gt; &lt;span class=&quot;bp&quot;&gt;True&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;span class=&quot;p&quot;&gt;])&lt;/span&gt;

&lt;span class=&quot;c1&quot;&gt;# Read from Accumulo
&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;df&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;spark&lt;/span&gt;
      &lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;read&lt;/span&gt;
      &lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;nb&quot;&gt;format&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&quot;com.microsoft.accumulo&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
      &lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;options&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;**&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;options&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;  &lt;span class=&quot;c1&quot;&gt;# define Accumulo properties
&lt;/span&gt;      &lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;schema&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;schema&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;))&lt;/span&gt;     &lt;span class=&quot;c1&quot;&gt;# define schema for data retrieval
&lt;/span&gt;
&lt;span class=&quot;c1&quot;&gt;# Write to Accumulo
&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;properties&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;'table'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;'output_table'&lt;/span&gt;

&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;df&lt;/span&gt;
 &lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;write&lt;/span&gt;
 &lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;nb&quot;&gt;format&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&quot;com.microsoft.accumulo&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
 &lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;options&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;**&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;options&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
 &lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;save&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;())&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;See the &lt;a href=&quot;https://github.com/microsoft/masc/blob/master/connector/examples/AccumuloSparkConnector.ipynb&quot;&gt;demo notebook&lt;/a&gt; for more examples.&lt;/p&gt;

&lt;h1 id=&quot;computational-performance-of-ai-scenario&quot;&gt;Computational Performance of AI Scenario&lt;/h1&gt;
&lt;h2 id=&quot;setup&quot;&gt;Setup&lt;/h2&gt;
&lt;p&gt;The benchmark setup used a 1,000-node Accumulo 2.0.0 Cluster (16,000 cores) running and a 256-node Spark 2.4.3 cluster (4,096 cores). All nodes used &lt;a href=&quot;https://docs.microsoft.com/en-us/azure/virtual-machines/windows/sizes-general&quot;&gt;Azure D16s_v3&lt;/a&gt; (16 cores) virtual machines. &lt;a href=&quot;https://github.com/apache/fluo-muchos&quot;&gt;Fluo-muchos&lt;/a&gt; was used to handle Accumulo and Spark cluster deployments and configuration.&lt;/p&gt;

&lt;p&gt;In all experiments we use the same base dataset which is a collection of Twitter user tweets with labeled sentiment value. This dataset is known as the Sentiment140 dataset (&lt;a href=&quot;https://www-nlp.stanford.edu/courses/cs224n/2009/fp/3.pdf&quot;&gt;Go, Bhayani, &amp;amp; Huang, 2009&lt;/a&gt;). The training data consist of 1.6M samples of tweets, where each tweet has columns indicating the sentiment label, user, timestamp, query term, and text. The text is limited to 140 characters and the overall uncompressed size of the training dataset is 227MB.&lt;/p&gt;

&lt;table&gt;
  &lt;thead&gt;
    &lt;tr&gt;
      &lt;th&gt;sentiment&lt;/th&gt;
      &lt;th&gt;id&lt;/th&gt;
      &lt;th&gt;date&lt;/th&gt;
      &lt;th&gt;query_string&lt;/th&gt;
      &lt;th&gt;user&lt;/th&gt;
      &lt;th&gt;text&lt;/th&gt;
    &lt;/tr&gt;
  &lt;/thead&gt;
  &lt;tbody&gt;
    &lt;tr&gt;
      &lt;td&gt;0&lt;/td&gt;
      &lt;td&gt;1467810369&lt;/td&gt;
      &lt;td&gt;Mon Apr 06 22:19:…&lt;/td&gt;
      &lt;td&gt;NO_QUERY&lt;/td&gt;
      &lt;td&gt;&lt;em&gt;TheSpecialOne&lt;/em&gt;&lt;/td&gt;
      &lt;td&gt;@switchfoot http:…&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;0&lt;/td&gt;
      &lt;td&gt;1467810672&lt;/td&gt;
      &lt;td&gt;Mon Apr 06 22:19:…&lt;/td&gt;
      &lt;td&gt;NO_QUERY&lt;/td&gt;
      &lt;td&gt;scotthamilton&lt;/td&gt;
      &lt;td&gt;is upset that he …&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;0&lt;/td&gt;
      &lt;td&gt;1467810917&lt;/td&gt;
      &lt;td&gt;Mon Apr 06 22:19:…&lt;/td&gt;
      &lt;td&gt;NO_QUERY&lt;/td&gt;
      &lt;td&gt;mattycus&lt;/td&gt;
      &lt;td&gt;@Kenichan I dived…&lt;/td&gt;
    &lt;/tr&gt;
  &lt;/tbody&gt;
&lt;/table&gt;

&lt;p&gt;To evaluate different table sizes and the impact of splitting the following procedure was used to generate the Accumulo tables:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;Prefix id with split keys (e.g. 0000, 0001, …, 1024)&lt;/li&gt;
  &lt;li&gt;Create Accumulo table and configure splits&lt;/li&gt;
  &lt;li&gt;Upload prefixed data to Accumulo using Spark and the MASC writer&lt;/li&gt;
  &lt;li&gt;Duplicate data using custom Accumulo server-side iterator&lt;/li&gt;
  &lt;li&gt;Validate data partitioning&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;A common machine learning scenario was evaluated using a sentiment model trained using &lt;a href=&quot;https://spark.apache.org/docs/latest/ml-guide.html&quot;&gt;SparkML&lt;/a&gt;. 
To train the classification model, we generated feature vectors from the text of tweets (text column). We used a feature engineering pipeline (a.k.a. featurizer) that breaks the text into tokens, splitting on whitespaces and discarding any capitalization and non-alphabetical characters. The pipeline consisted of&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;Regex Tokenizer&lt;/li&gt;
  &lt;li&gt;Hashing Transformer&lt;/li&gt;
  &lt;li&gt;Logistic Regression&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;See the &lt;a href=&quot;https://github.com/microsoft/masc/blob/master/connector/examples/AccumuloSparkConnectorBenchmark.ipynb&quot;&gt;benchmark notebook (Scala)&lt;/a&gt; for more details.&lt;/p&gt;

&lt;h2 id=&quot;results&quot;&gt;Results&lt;/h2&gt;
&lt;p&gt;The first set of experiments evaluated data transfer efficiency and ML model inference performance. The chart below shows&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;Accumulo table split size (1GB, 8GB, 32GB, 64GB)&lt;/li&gt;
  &lt;li&gt;Total table size (1TB, 10TB, 100TB, 1PB)&lt;/li&gt;
  &lt;li&gt;Operations
    &lt;ul&gt;
      &lt;li&gt;Count: plain count of the data&lt;/li&gt;
      &lt;li&gt;Inference: Accumulo server-side inference using MLeap&lt;/li&gt;
      &lt;li&gt;Transfer: Filtering results for 30% data transfer&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;Time is reported in minutes&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;Remarks&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;Time is log-scale&lt;/li&gt;
  &lt;li&gt;Inference was run with and without data transfer to isolate server-side performance.&lt;/li&gt;
  &lt;li&gt;The smaller each Accumulo table split is, the more splits we have and thus higher parallelization.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;img src=&quot;/images/blog/202002_masc/runtime.png&quot; alt=&quot;Runtime&quot; title=&quot;Runtime Performance&quot; class=&quot;blog-img-center&quot; /&gt;&lt;/p&gt;

&lt;p&gt;The second set of experiments highlights the computational performance improvement of using the server-side inference approach compared to running inference on the Spark cluster.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/images/blog/202002_masc/sparkml_vs_mleap_accumulo.png&quot; alt=&quot;Mleap&quot; title=&quot;Spark ML vs MLeap Performance&quot; class=&quot;blog-img-center&quot; /&gt;&lt;/p&gt;

&lt;h1 id=&quot;learnings&quot;&gt;Learnings&lt;/h1&gt;
&lt;ul&gt;
  &lt;li&gt;Accumulo MLeap Server-side inference vs Spark ML results in a 2x improvement&lt;/li&gt;
  &lt;li&gt;Multi-threading in Spark jobs can be used to fully utilize Accumulo servers
    &lt;ul&gt;
      &lt;li&gt;Useful when Spark cluster has less cores than Accumulo&lt;/li&gt;
      &lt;li&gt;e.g. 8 threads * 2,048 Spark executor = 16,384 Accumulo threads&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;Unbalanced Accumulo table splits can introduce performance bottlenecks&lt;/li&gt;
&lt;/ul&gt;

&lt;h1 id=&quot;useful-links&quot;&gt;Useful links&lt;/h1&gt;
&lt;ul&gt;
  &lt;li&gt;&lt;a href=&quot;https://github.com/microsoft/masc/blob/master/connector/examples/AccumuloSparkConnector.ipynb&quot;&gt;Complete Jupyter demo notebook (PySpark)&lt;/a&gt; for usage of the Accumulo-Spark connector&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;https://github.com/microsoft/masc/blob/master/connector/examples/AccumuloSparkConnectorBenchmark.ipynb&quot;&gt;Complete Jupyter benchmark notebook (Scala)&lt;/a&gt; for usage of the Accumulo-Spark connector&lt;/li&gt;
  &lt;li&gt;GitHub Repository &lt;a href=&quot;https://github.com/microsoft/masc&quot;&gt;Microsoft’s contributions for Spark with Apache Accumulo&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;https://github.com/combust/mleap&quot;&gt;MLeap&lt;/a&gt; - Scala/Java stand-alone model inference for SparkML-based models&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;https://spark.apache.org/docs/latest/ml-guide.html&quot;&gt;SparkML&lt;/a&gt; - Spark machine learning library&lt;/li&gt;
  &lt;li&gt;MASC Maven artifacts
    &lt;ul&gt;
      &lt;li&gt;&lt;a href=&quot;https://mvnrepository.com/artifact/com.microsoft.masc/microsoft-accumulo-spark-iterator&quot;&gt;Accumulo Iterator - Backend for Spark DataSource&lt;/a&gt;&lt;/li&gt;
      &lt;li&gt;&lt;a href=&quot;https://mvnrepository.com/artifact/com.microsoft.masc/microsoft-accumulo-spark-datasource&quot;&gt;Spark DataSource&lt;/a&gt;&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;h1 id=&quot;license&quot;&gt;License&lt;/h1&gt;
&lt;p&gt;This work is publicly available under the Apache License 2.0 on GitHub under &lt;a href=&quot;https://github.com/microsoft/masc&quot;&gt;Microsoft’s contributions for Apache Spark with Apache Accumulo&lt;/a&gt;.&lt;/p&gt;

&lt;h1 id=&quot;contributions&quot;&gt;Contributions&lt;/h1&gt;
&lt;p&gt;Feedback, questions, and contributions are welcome!&lt;/p&gt;

&lt;p&gt;Thanks to contributions from members on the Azure Global Customer Engineering and Azure Government teams.&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;&lt;a href=&quot;https://github.com/AnupamMicrosoft&quot;&gt;Anupam Sharma&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;https://github.com/arvindshmicrosoft&quot;&gt;Arvind Shyamsundar&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;https://github.com/billierinaldi&quot;&gt;Billie Rinaldi&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;https://github.com/chenhuims&quot;&gt;Chenhui Hu&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;https://github.com/loomlike&quot;&gt;Jun-Ki Min&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;https://github.com/phrocker&quot;&gt;Marc Parisi&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;https://github.com/eisber&quot;&gt;Markus Cozowicz&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;Pavandeep Kalra&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;https://github.com/roalexan&quot;&gt;Robert Alexander&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;https://github.com/gramhagen&quot;&gt;Scott Graham&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;https://github.com/wutaomsft&quot;&gt;Tao Wu&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;Special thanks to &lt;a href=&quot;https://github.com/ancasarb&quot;&gt;Anca Sarb&lt;/a&gt; for promptly assisting with &lt;a href=&quot;https://github.com/combust/mleap/issues/633&quot;&gt;MLeap performance issues&lt;/a&gt;.&lt;/p&gt;
</description>
        <pubDate>Wed, 26 Feb 2020 00:00:00 +0000</pubDate>
        <link>https://accumulo.apache.org/blog/2020/02/26/accumulo-spark-connector.html</link>
        <guid isPermaLink="true">https://accumulo.apache.org/blog/2020/02/26/accumulo-spark-connector.html</guid>
        
        
        <category>blog</category>
        
      </item>
    
      <item>
        <title>Accumulo Clients in Other Programming Languages</title>
        <description>&lt;p&gt;Apache Accumulo has an &lt;a href=&quot;https://github.com/apache/accumulo-proxy&quot;&gt;Accumulo Proxy&lt;/a&gt; that allows communication with Accumulo using clients written
in languages other than Java. This blog post shows how to run the Accumulo Proxy process using &lt;a href=&quot;https://github.com/apache/fluo-uno&quot;&gt;Uno&lt;/a&gt;
and communicate with Accumulo using a Python client.&lt;/p&gt;

&lt;p&gt;First, clone the &lt;a href=&quot;https://github.com/apache/accumulo-proxy&quot;&gt;Accumulo Proxy&lt;/a&gt; repository.&lt;/p&gt;

&lt;div class=&quot;language-bash highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;git clone https://github.com/apache/accumulo-proxy
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;Assuming you have &lt;a href=&quot;https://github.com/apache/fluo-uno&quot;&gt;Uno&lt;/a&gt; set up on your machine, configure &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;uno.conf&lt;/code&gt; to start the &lt;a href=&quot;https://github.com/apache/accumulo-proxy&quot;&gt;Accumulo Proxy&lt;/a&gt;
by setting the configuration below:&lt;/p&gt;

&lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;export POST_RUN_PLUGINS=&quot;accumulo-proxy&quot;
export PROXY_REPO=/path/to/accumulo-proxy
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;Run the following command to set up Accumulo again. The Proxy will be started after Accumulo runs.&lt;/p&gt;

&lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;uno setup accumulo
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;After Accumulo is set up, you should see the following output from uno:&lt;/p&gt;

&lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;Executing post run plugin: accumulo-proxy
Installing Accumulo Proxy at /path/to/fluo-uno/install/accumulo-proxy-2.0.0-SNAPSHOT
Accumulo Proxy 2.0.0-SNAPSHOT is running
    * view logs at /path/to/fluo-uno/install/logs/accumulo-proxy/
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;Next, follow the instructions below to create a Python 2.7 client that creates an Accumulo table
named &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;pythontest&lt;/code&gt; and writes data to it:&lt;/p&gt;

&lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;mkdir accumulo-client/
cd accumulo-client/
pipenv --python 2.7
pipenv install thrift
pipenv install -e /path/to/accumulo-proxy/src/main/python
cp /path/to/accumulo-proxy/src/main/python/basic_client.py .
# Edit credentials if needed
vim basic_client.py
pipenv run python2 basic_client.py
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;Verify that the table was created or data was written using &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;uno ashell&lt;/code&gt; or the Accumulo monitor.&lt;/p&gt;

</description>
        <pubDate>Mon, 16 Dec 2019 00:00:00 +0000</pubDate>
        <link>https://accumulo.apache.org/blog/2019/12/16/accumulo-proxy.html</link>
        <guid isPermaLink="true">https://accumulo.apache.org/blog/2019/12/16/accumulo-proxy.html</guid>
        
        
        <category>blog</category>
        
      </item>
    
      <item>
        <title>Checking API use</title>
        <description>&lt;p&gt;Accumulo follows &lt;a href=&quot;https://semver.org/&quot;&gt;SemVer&lt;/a&gt; across versions with the declaration of a public API.  Code not in the public API should be
considered unstable, at risk of changing between versions.  The public API packages are &lt;a href=&quot;/api/&quot;&gt;listed on the website&lt;/a&gt;
but may not be considered when an Accumulo user writes code.  This blog post explains how to make Maven
automatically detect usage of Accumulo code outside the public API.&lt;/p&gt;

&lt;p&gt;The techniques described in this blog post only work for Accumulo 2.0 and later.  Do not use with 1.X versions.&lt;/p&gt;

&lt;h2 id=&quot;checkstyle-plugin&quot;&gt;Checkstyle Plugin&lt;/h2&gt;

&lt;p&gt;First add the checkstyle Maven plugin to your pom.&lt;/p&gt;

&lt;div class=&quot;language-xml highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;nt&quot;&gt;&amp;lt;plugin&amp;gt;&lt;/span&gt;
    &lt;span class=&quot;c&quot;&gt;&amp;lt;!-- This was added to ensure project only uses Accumulo's public API --&amp;gt;&lt;/span&gt;
    &lt;span class=&quot;nt&quot;&gt;&amp;lt;groupId&amp;gt;&lt;/span&gt;org.apache.maven.plugins&lt;span class=&quot;nt&quot;&gt;&amp;lt;/groupId&amp;gt;&lt;/span&gt;
    &lt;span class=&quot;nt&quot;&gt;&amp;lt;artifactId&amp;gt;&lt;/span&gt;maven-checkstyle-plugin&lt;span class=&quot;nt&quot;&gt;&amp;lt;/artifactId&amp;gt;&lt;/span&gt;
    &lt;span class=&quot;nt&quot;&gt;&amp;lt;version&amp;gt;&lt;/span&gt;3.1.0&lt;span class=&quot;nt&quot;&gt;&amp;lt;/version&amp;gt;&lt;/span&gt;
    &lt;span class=&quot;nt&quot;&gt;&amp;lt;executions&amp;gt;&lt;/span&gt;
      &lt;span class=&quot;nt&quot;&gt;&amp;lt;execution&amp;gt;&lt;/span&gt;
        &lt;span class=&quot;nt&quot;&gt;&amp;lt;id&amp;gt;&lt;/span&gt;check-style&lt;span class=&quot;nt&quot;&gt;&amp;lt;/id&amp;gt;&lt;/span&gt;
        &lt;span class=&quot;nt&quot;&gt;&amp;lt;goals&amp;gt;&lt;/span&gt;
          &lt;span class=&quot;nt&quot;&gt;&amp;lt;goal&amp;gt;&lt;/span&gt;check&lt;span class=&quot;nt&quot;&gt;&amp;lt;/goal&amp;gt;&lt;/span&gt;
        &lt;span class=&quot;nt&quot;&gt;&amp;lt;/goals&amp;gt;&lt;/span&gt;
        &lt;span class=&quot;nt&quot;&gt;&amp;lt;configuration&amp;gt;&lt;/span&gt;
          &lt;span class=&quot;nt&quot;&gt;&amp;lt;configLocation&amp;gt;&lt;/span&gt;checkstyle.xml&lt;span class=&quot;nt&quot;&gt;&amp;lt;/configLocation&amp;gt;&lt;/span&gt;
        &lt;span class=&quot;nt&quot;&gt;&amp;lt;/configuration&amp;gt;&lt;/span&gt;
      &lt;span class=&quot;nt&quot;&gt;&amp;lt;/execution&amp;gt;&lt;/span&gt;
    &lt;span class=&quot;nt&quot;&gt;&amp;lt;/executions&amp;gt;&lt;/span&gt;
  &lt;span class=&quot;nt&quot;&gt;&amp;lt;/plugin&amp;gt;&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;
&lt;p&gt;The plugin version is the latest at the time of this post.  For more information see the website for
the &lt;a href=&quot;https://maven.apache.org/plugins/maven-checkstyle-plugin/&quot;&gt;Apache Maven Checkstyle Plugin&lt;/a&gt;.  The configuration above adds the plugin to &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;check&lt;/code&gt; execution goal
so it will always run with your build.&lt;/p&gt;

&lt;p&gt;Create the configuration file specified above: &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;checkstyle.xml&lt;/code&gt;&lt;/p&gt;

&lt;h3 id=&quot;checkstylexml&quot;&gt;checkstyle.xml&lt;/h3&gt;

&lt;div class=&quot;language-xml highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;cp&quot;&gt;&amp;lt;!DOCTYPE module PUBLIC &quot;-//Puppy Crawl//DTD Check Configuration 1.3//EN&quot; &quot;http://www.puppycrawl.com/dtds/configuration_1_3.dtd&quot;&amp;gt;&lt;/span&gt;
&lt;span class=&quot;nt&quot;&gt;&amp;lt;module&lt;/span&gt; &lt;span class=&quot;na&quot;&gt;name=&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&quot;Checker&quot;&lt;/span&gt;&lt;span class=&quot;nt&quot;&gt;&amp;gt;&lt;/span&gt;
  &lt;span class=&quot;nt&quot;&gt;&amp;lt;property&lt;/span&gt; &lt;span class=&quot;na&quot;&gt;name=&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&quot;charset&quot;&lt;/span&gt; &lt;span class=&quot;na&quot;&gt;value=&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&quot;UTF-8&quot;&lt;/span&gt;&lt;span class=&quot;nt&quot;&gt;/&amp;gt;&lt;/span&gt;
  &lt;span class=&quot;nt&quot;&gt;&amp;lt;module&lt;/span&gt; &lt;span class=&quot;na&quot;&gt;name=&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&quot;TreeWalker&quot;&lt;/span&gt;&lt;span class=&quot;nt&quot;&gt;&amp;gt;&lt;/span&gt;
    &lt;span class=&quot;c&quot;&gt;&amp;lt;!--check that only Accumulo public APIs are imported--&amp;gt;&lt;/span&gt;
    &lt;span class=&quot;nt&quot;&gt;&amp;lt;module&lt;/span&gt; &lt;span class=&quot;na&quot;&gt;name=&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&quot;ImportControl&quot;&lt;/span&gt;&lt;span class=&quot;nt&quot;&gt;&amp;gt;&lt;/span&gt;
      &lt;span class=&quot;nt&quot;&gt;&amp;lt;property&lt;/span&gt; &lt;span class=&quot;na&quot;&gt;name=&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&quot;file&quot;&lt;/span&gt; &lt;span class=&quot;na&quot;&gt;value=&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&quot;import-control.xml&quot;&lt;/span&gt;&lt;span class=&quot;nt&quot;&gt;/&amp;gt;&lt;/span&gt;
    &lt;span class=&quot;nt&quot;&gt;&amp;lt;/module&amp;gt;&lt;/span&gt;
  &lt;span class=&quot;nt&quot;&gt;&amp;lt;/module&amp;gt;&lt;/span&gt;
&lt;span class=&quot;nt&quot;&gt;&amp;lt;/module&amp;gt;&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;
&lt;p&gt;This file sets up the ImportControl module.&lt;/p&gt;

&lt;h2 id=&quot;import-control-configuration&quot;&gt;Import Control Configuration&lt;/h2&gt;

&lt;p&gt;Create the second file specified above, &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;import-control.xml&lt;/code&gt; and copy the configuration below.  Make sure to replace
“insert-your-package-name” with the package name of your project.&lt;/p&gt;
&lt;div class=&quot;language-xml highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;cp&quot;&gt;&amp;lt;!DOCTYPE import-control PUBLIC
    &quot;-//Checkstyle//DTD ImportControl Configuration 1.4//EN&quot;
    &quot;https://checkstyle.org/dtds/import_control_1_4.dtd&quot;&amp;gt;&lt;/span&gt;

&lt;span class=&quot;c&quot;&gt;&amp;lt;!-- This checkstyle rule is configured to ensure only use of Accumulo API --&amp;gt;&lt;/span&gt;
&lt;span class=&quot;nt&quot;&gt;&amp;lt;import-control&lt;/span&gt; &lt;span class=&quot;na&quot;&gt;pkg=&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&quot;insert-your-package-name&quot;&lt;/span&gt; &lt;span class=&quot;na&quot;&gt;strategyOnMismatch=&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&quot;allowed&quot;&lt;/span&gt;&lt;span class=&quot;nt&quot;&gt;&amp;gt;&lt;/span&gt;
    &lt;span class=&quot;c&quot;&gt;&amp;lt;!-- API packages --&amp;gt;&lt;/span&gt;
    &lt;span class=&quot;nt&quot;&gt;&amp;lt;allow&lt;/span&gt; &lt;span class=&quot;na&quot;&gt;pkg=&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&quot;org.apache.accumulo.core.client&quot;&lt;/span&gt;&lt;span class=&quot;nt&quot;&gt;/&amp;gt;&lt;/span&gt;
    &lt;span class=&quot;nt&quot;&gt;&amp;lt;allow&lt;/span&gt; &lt;span class=&quot;na&quot;&gt;pkg=&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&quot;org.apache.accumulo.core.data&quot;&lt;/span&gt;&lt;span class=&quot;nt&quot;&gt;/&amp;gt;&lt;/span&gt;
    &lt;span class=&quot;nt&quot;&gt;&amp;lt;allow&lt;/span&gt; &lt;span class=&quot;na&quot;&gt;pkg=&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&quot;org.apache.accumulo.core.security&quot;&lt;/span&gt;&lt;span class=&quot;nt&quot;&gt;/&amp;gt;&lt;/span&gt;
    &lt;span class=&quot;nt&quot;&gt;&amp;lt;allow&lt;/span&gt; &lt;span class=&quot;na&quot;&gt;pkg=&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&quot;org.apache.accumulo.core.iterators&quot;&lt;/span&gt;&lt;span class=&quot;nt&quot;&gt;/&amp;gt;&lt;/span&gt;
    &lt;span class=&quot;nt&quot;&gt;&amp;lt;allow&lt;/span&gt; &lt;span class=&quot;na&quot;&gt;pkg=&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&quot;org.apache.accumulo.minicluster&quot;&lt;/span&gt;&lt;span class=&quot;nt&quot;&gt;/&amp;gt;&lt;/span&gt;
    &lt;span class=&quot;nt&quot;&gt;&amp;lt;allow&lt;/span&gt; &lt;span class=&quot;na&quot;&gt;pkg=&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&quot;org.apache.accumulo.hadoop.mapreduce&quot;&lt;/span&gt;&lt;span class=&quot;nt&quot;&gt;/&amp;gt;&lt;/span&gt;

    &lt;span class=&quot;c&quot;&gt;&amp;lt;!-- disallow everything else coming from accumulo --&amp;gt;&lt;/span&gt;
    &lt;span class=&quot;nt&quot;&gt;&amp;lt;disallow&lt;/span&gt; &lt;span class=&quot;na&quot;&gt;pkg=&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&quot;org.apache.accumulo&quot;&lt;/span&gt;&lt;span class=&quot;nt&quot;&gt;/&amp;gt;&lt;/span&gt;
&lt;span class=&quot;nt&quot;&gt;&amp;lt;/import-control&amp;gt;&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;
&lt;p&gt;This file configures the ImportControl module to only allow packages that are declared public API.&lt;/p&gt;

&lt;h2 id=&quot;hold-the-line&quot;&gt;Hold the line&lt;/h2&gt;

&lt;p&gt;Adding this to an existing project may expose usage of non public Accumulo API’s. It may take more time than is available
to fix those at first, but do not let this discourage adding this plugin. One possible way to proceed is to allow the
currently used non-public APIs in a commented section of import-control.xml noting these are temporarily allowed until
they can be removed. This strategy prevents new usages of non-public APIs while allowing time to work on fixing the current
 usages of non public APIs.  Also, if you don’t want your project failing to build because of this, you can add &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;&amp;lt;failOnViolation&amp;gt;false&amp;lt;/failOnViolation&amp;gt;&lt;/code&gt;
to the maven-checkstyle-plugin configuration.&lt;/p&gt;

</description>
        <pubDate>Mon, 04 Nov 2019 00:00:00 +0000</pubDate>
        <link>https://accumulo.apache.org/blog/2019/11/04/checkstyle-import-control.html</link>
        <guid isPermaLink="true">https://accumulo.apache.org/blog/2019/11/04/checkstyle-import-control.html</guid>
        
        
        <category>blog</category>
        
      </item>
    
      <item>
        <title>Using Azure Data Lake Gen2 storage as a data store for Accumulo</title>
        <description>&lt;p&gt;Accumulo can store its files in &lt;a href=&quot;https://docs.microsoft.com/en-us/azure/storage/blobs/data-lake-storage-introduction&quot;&gt;Azure Data Lake Storage Gen2&lt;/a&gt;
using the &lt;a href=&quot;https://docs.microsoft.com/en-us/azure/storage/blobs/data-lake-storage-abfs-driver&quot;&gt;ABFS (Azure Blob File System)&lt;/a&gt; driver.
Similar to &lt;a href=&quot;/blog/2019/09/10/accumulo-S3-notes.html&quot;&gt;S3 blog&lt;/a&gt;,
the write ahead logs &amp;amp; Accumulo metadata can be stored in HDFS and everything else on Gen2 storage
using the volume chooser feature introduced in Accumulo 2.0. The configurations referred on this blog
are specific to Accumulo 2.0 and Hadoop 3.2.0.&lt;/p&gt;

&lt;h2 id=&quot;hadoop-setup&quot;&gt;Hadoop setup&lt;/h2&gt;

&lt;p&gt;For ABFS client to talk to Gen2 storage, it requires one of the Authentication mechanism listed &lt;a href=&quot;https://hadoop.apache.org/docs/current/hadoop-azure/abfs.html#Authentication&quot;&gt;here&lt;/a&gt;
This post covers &lt;a href=&quot;https://docs.microsoft.com/en-us/azure/active-directory/managed-identities-azure-resources/overview&quot;&gt;Azure Managed Identity&lt;/a&gt;
formerly known as Managed Service Identity or MSI. This feature provides Azure services with an 
automatically managed identity in &lt;a href=&quot;https://docs.microsoft.com/en-us/azure/active-directory/fundamentals/active-directory-whatis&quot;&gt;Azure AD&lt;/a&gt;
and it avoids the need for credentials or other sensitive information from being stored in code 
or configs/JCEKS. Plus, it comes free with Azure AD.&lt;/p&gt;

&lt;p&gt;At least the following should be added to Hadoop’s &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;core-site.xml&lt;/code&gt; on each node.&lt;/p&gt;

&lt;div class=&quot;language-xml highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;nt&quot;&gt;&amp;lt;property&amp;gt;&lt;/span&gt;
  &lt;span class=&quot;nt&quot;&gt;&amp;lt;name&amp;gt;&lt;/span&gt;fs.azure.account.auth.type&lt;span class=&quot;nt&quot;&gt;&amp;lt;/name&amp;gt;&lt;/span&gt;
  &lt;span class=&quot;nt&quot;&gt;&amp;lt;value&amp;gt;&lt;/span&gt;OAuth&lt;span class=&quot;nt&quot;&gt;&amp;lt;/value&amp;gt;&lt;/span&gt;
&lt;span class=&quot;nt&quot;&gt;&amp;lt;/property&amp;gt;&lt;/span&gt;
&lt;span class=&quot;nt&quot;&gt;&amp;lt;property&amp;gt;&lt;/span&gt;
  &lt;span class=&quot;nt&quot;&gt;&amp;lt;name&amp;gt;&lt;/span&gt;fs.azure.account.oauth.provider.type&lt;span class=&quot;nt&quot;&gt;&amp;lt;/name&amp;gt;&lt;/span&gt;
  &lt;span class=&quot;nt&quot;&gt;&amp;lt;value&amp;gt;&lt;/span&gt;org.apache.hadoop.fs.azurebfs.oauth2.MsiTokenProvider&lt;span class=&quot;nt&quot;&gt;&amp;lt;/value&amp;gt;&lt;/span&gt;
&lt;span class=&quot;nt&quot;&gt;&amp;lt;/property&amp;gt;&lt;/span&gt;
&lt;span class=&quot;nt&quot;&gt;&amp;lt;property&amp;gt;&lt;/span&gt;
  &lt;span class=&quot;nt&quot;&gt;&amp;lt;name&amp;gt;&lt;/span&gt;fs.azure.account.oauth2.msi.tenant&lt;span class=&quot;nt&quot;&gt;&amp;lt;/name&amp;gt;&lt;/span&gt;
  &lt;span class=&quot;nt&quot;&gt;&amp;lt;value&amp;gt;&lt;/span&gt;TenantID&lt;span class=&quot;nt&quot;&gt;&amp;lt;/value&amp;gt;&lt;/span&gt;
&lt;span class=&quot;nt&quot;&gt;&amp;lt;/property&amp;gt;&lt;/span&gt;
&lt;span class=&quot;nt&quot;&gt;&amp;lt;property&amp;gt;&lt;/span&gt;
  &lt;span class=&quot;nt&quot;&gt;&amp;lt;name&amp;gt;&lt;/span&gt;fs.azure.account.oauth2.client.id&lt;span class=&quot;nt&quot;&gt;&amp;lt;/name&amp;gt;&lt;/span&gt;
  &lt;span class=&quot;nt&quot;&gt;&amp;lt;value&amp;gt;&lt;/span&gt;ClientID&lt;span class=&quot;nt&quot;&gt;&amp;lt;/value&amp;gt;&lt;/span&gt;
&lt;span class=&quot;nt&quot;&gt;&amp;lt;/property&amp;gt;&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;See &lt;a href=&quot;https://hadoop.apache.org/docs/current/hadoop-azure/abfs.html&quot;&gt;ABFS doc&lt;/a&gt;
for more information on Hadoop Azure support.&lt;/p&gt;

&lt;p&gt;To get hadoop command to work with ADLS Gen2 set the 
following entries in &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;hadoop-env.sh&lt;/code&gt;. As Gen2 storage is TLS enabled by default, 
it is important we use the native OpenSSL implementation of TLS.&lt;/p&gt;

&lt;div class=&quot;language-bash highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;nb&quot;&gt;export &lt;/span&gt;&lt;span class=&quot;nv&quot;&gt;HADOOP_OPTIONAL_TOOLS&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;s2&quot;&gt;&quot;hadoop-azure&quot;&lt;/span&gt;
&lt;span class=&quot;nb&quot;&gt;export &lt;/span&gt;&lt;span class=&quot;nv&quot;&gt;HADOOP_OPTS&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;s2&quot;&gt;&quot;-Dorg.wildfly.openssl.path=&amp;lt;path/to/OpenSSL/libraries&amp;gt; &lt;/span&gt;&lt;span class=&quot;k&quot;&gt;${&lt;/span&gt;&lt;span class=&quot;nv&quot;&gt;HADOOP_OPTS&lt;/span&gt;&lt;span class=&quot;k&quot;&gt;}&lt;/span&gt;&lt;span class=&quot;s2&quot;&gt;&quot;&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;To verify the location of the OpenSSL libraries, run &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;whereis libssl&lt;/code&gt; command 
on the host&lt;/p&gt;

&lt;h2 id=&quot;accumulo-setup&quot;&gt;Accumulo setup&lt;/h2&gt;

&lt;p&gt;For each node in the cluster, modify &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;accumulo-env.sh&lt;/code&gt; to add Azure storage jars to the
classpath.  Your versions may differ depending on your Hadoop version,
following versions were included with Hadoop 3.2.0.&lt;/p&gt;

&lt;div class=&quot;language-bash highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;nv&quot;&gt;CLASSPATH&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;s2&quot;&gt;&quot;&lt;/span&gt;&lt;span class=&quot;k&quot;&gt;${&lt;/span&gt;&lt;span class=&quot;nv&quot;&gt;conf&lt;/span&gt;&lt;span class=&quot;k&quot;&gt;}&lt;/span&gt;&lt;span class=&quot;s2&quot;&gt;:&lt;/span&gt;&lt;span class=&quot;k&quot;&gt;${&lt;/span&gt;&lt;span class=&quot;nv&quot;&gt;lib&lt;/span&gt;&lt;span class=&quot;k&quot;&gt;}&lt;/span&gt;&lt;span class=&quot;s2&quot;&gt;/*:&lt;/span&gt;&lt;span class=&quot;k&quot;&gt;${&lt;/span&gt;&lt;span class=&quot;nv&quot;&gt;HADOOP_CONF_DIR&lt;/span&gt;&lt;span class=&quot;k&quot;&gt;}&lt;/span&gt;&lt;span class=&quot;s2&quot;&gt;:&lt;/span&gt;&lt;span class=&quot;k&quot;&gt;${&lt;/span&gt;&lt;span class=&quot;nv&quot;&gt;ZOOKEEPER_HOME&lt;/span&gt;&lt;span class=&quot;k&quot;&gt;}&lt;/span&gt;&lt;span class=&quot;s2&quot;&gt;/*:&lt;/span&gt;&lt;span class=&quot;k&quot;&gt;${&lt;/span&gt;&lt;span class=&quot;nv&quot;&gt;HADOOP_HOME&lt;/span&gt;&lt;span class=&quot;k&quot;&gt;}&lt;/span&gt;&lt;span class=&quot;s2&quot;&gt;/share/hadoop/client/*&quot;&lt;/span&gt;
&lt;span class=&quot;nv&quot;&gt;CLASSPATH&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;s2&quot;&gt;&quot;&lt;/span&gt;&lt;span class=&quot;k&quot;&gt;${&lt;/span&gt;&lt;span class=&quot;nv&quot;&gt;CLASSPATH&lt;/span&gt;&lt;span class=&quot;k&quot;&gt;}&lt;/span&gt;&lt;span class=&quot;s2&quot;&gt;:&lt;/span&gt;&lt;span class=&quot;k&quot;&gt;${&lt;/span&gt;&lt;span class=&quot;nv&quot;&gt;HADOOP_HOME&lt;/span&gt;&lt;span class=&quot;k&quot;&gt;}&lt;/span&gt;&lt;span class=&quot;s2&quot;&gt;/share/hadoop/tools/lib/azure-data-lake-store-sdk-2.2.9.jar&quot;&lt;/span&gt;
&lt;span class=&quot;nv&quot;&gt;CLASSPATH&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;s2&quot;&gt;&quot;&lt;/span&gt;&lt;span class=&quot;k&quot;&gt;${&lt;/span&gt;&lt;span class=&quot;nv&quot;&gt;CLASSPATH&lt;/span&gt;&lt;span class=&quot;k&quot;&gt;}&lt;/span&gt;&lt;span class=&quot;s2&quot;&gt;:&lt;/span&gt;&lt;span class=&quot;k&quot;&gt;${&lt;/span&gt;&lt;span class=&quot;nv&quot;&gt;HADOOP_HOME&lt;/span&gt;&lt;span class=&quot;k&quot;&gt;}&lt;/span&gt;&lt;span class=&quot;s2&quot;&gt;/share/hadoop/tools/lib/azure-keyvault-core-1.0.0.jar&quot;&lt;/span&gt;
&lt;span class=&quot;nv&quot;&gt;CLASSPATH&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;s2&quot;&gt;&quot;&lt;/span&gt;&lt;span class=&quot;k&quot;&gt;${&lt;/span&gt;&lt;span class=&quot;nv&quot;&gt;CLASSPATH&lt;/span&gt;&lt;span class=&quot;k&quot;&gt;}&lt;/span&gt;&lt;span class=&quot;s2&quot;&gt;:&lt;/span&gt;&lt;span class=&quot;k&quot;&gt;${&lt;/span&gt;&lt;span class=&quot;nv&quot;&gt;HADOOP_HOME&lt;/span&gt;&lt;span class=&quot;k&quot;&gt;}&lt;/span&gt;&lt;span class=&quot;s2&quot;&gt;/share/hadoop/tools/lib/hadoop-azure-3.2.0.jar&quot;&lt;/span&gt;
&lt;span class=&quot;nv&quot;&gt;CLASSPATH&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;s2&quot;&gt;&quot;&lt;/span&gt;&lt;span class=&quot;k&quot;&gt;${&lt;/span&gt;&lt;span class=&quot;nv&quot;&gt;CLASSPATH&lt;/span&gt;&lt;span class=&quot;k&quot;&gt;}&lt;/span&gt;&lt;span class=&quot;s2&quot;&gt;:&lt;/span&gt;&lt;span class=&quot;k&quot;&gt;${&lt;/span&gt;&lt;span class=&quot;nv&quot;&gt;HADOOP_HOME&lt;/span&gt;&lt;span class=&quot;k&quot;&gt;}&lt;/span&gt;&lt;span class=&quot;s2&quot;&gt;/share/hadoop/tools/lib/wildfly-openssl-1.0.4.Final.jar&quot;&lt;/span&gt;
&lt;span class=&quot;nv&quot;&gt;CLASSPATH&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;s2&quot;&gt;&quot;&lt;/span&gt;&lt;span class=&quot;k&quot;&gt;${&lt;/span&gt;&lt;span class=&quot;nv&quot;&gt;CLASSPATH&lt;/span&gt;&lt;span class=&quot;k&quot;&gt;}&lt;/span&gt;&lt;span class=&quot;s2&quot;&gt;:&lt;/span&gt;&lt;span class=&quot;k&quot;&gt;${&lt;/span&gt;&lt;span class=&quot;nv&quot;&gt;HADOOP_HOME&lt;/span&gt;&lt;span class=&quot;k&quot;&gt;}&lt;/span&gt;&lt;span class=&quot;s2&quot;&gt;/share/hadoop/common/lib/jaxb-api-2.2.11.jar&quot;&lt;/span&gt;
&lt;span class=&quot;nv&quot;&gt;CLASSPATH&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;s2&quot;&gt;&quot;&lt;/span&gt;&lt;span class=&quot;k&quot;&gt;${&lt;/span&gt;&lt;span class=&quot;nv&quot;&gt;CLASSPATH&lt;/span&gt;&lt;span class=&quot;k&quot;&gt;}&lt;/span&gt;&lt;span class=&quot;s2&quot;&gt;:&lt;/span&gt;&lt;span class=&quot;k&quot;&gt;${&lt;/span&gt;&lt;span class=&quot;nv&quot;&gt;HADOOP_HOME&lt;/span&gt;&lt;span class=&quot;k&quot;&gt;}&lt;/span&gt;&lt;span class=&quot;s2&quot;&gt;/share/hadoop/common/lib/jaxb-impl-2.2.3-1.jar&quot;&lt;/span&gt;
&lt;span class=&quot;nv&quot;&gt;CLASSPATH&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;s2&quot;&gt;&quot;&lt;/span&gt;&lt;span class=&quot;k&quot;&gt;${&lt;/span&gt;&lt;span class=&quot;nv&quot;&gt;CLASSPATH&lt;/span&gt;&lt;span class=&quot;k&quot;&gt;}&lt;/span&gt;&lt;span class=&quot;s2&quot;&gt;:&lt;/span&gt;&lt;span class=&quot;k&quot;&gt;${&lt;/span&gt;&lt;span class=&quot;nv&quot;&gt;HADOOP_HOME&lt;/span&gt;&lt;span class=&quot;k&quot;&gt;}&lt;/span&gt;&lt;span class=&quot;s2&quot;&gt;/share/hadoop/common/lib/commons-lang3-3.7.jar&quot;&lt;/span&gt;
&lt;span class=&quot;nv&quot;&gt;CLASSPATH&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;s2&quot;&gt;&quot;&lt;/span&gt;&lt;span class=&quot;k&quot;&gt;${&lt;/span&gt;&lt;span class=&quot;nv&quot;&gt;CLASSPATH&lt;/span&gt;&lt;span class=&quot;k&quot;&gt;}&lt;/span&gt;&lt;span class=&quot;s2&quot;&gt;:&lt;/span&gt;&lt;span class=&quot;k&quot;&gt;${&lt;/span&gt;&lt;span class=&quot;nv&quot;&gt;HADOOP_HOME&lt;/span&gt;&lt;span class=&quot;k&quot;&gt;}&lt;/span&gt;&lt;span class=&quot;s2&quot;&gt;/share/hadoop/common/lib/httpclient-4.5.2.jar&quot;&lt;/span&gt;
&lt;span class=&quot;nv&quot;&gt;CLASSPATH&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;s2&quot;&gt;&quot;&lt;/span&gt;&lt;span class=&quot;k&quot;&gt;${&lt;/span&gt;&lt;span class=&quot;nv&quot;&gt;CLASSPATH&lt;/span&gt;&lt;span class=&quot;k&quot;&gt;}&lt;/span&gt;&lt;span class=&quot;s2&quot;&gt;:&lt;/span&gt;&lt;span class=&quot;k&quot;&gt;${&lt;/span&gt;&lt;span class=&quot;nv&quot;&gt;HADOOP_HOME&lt;/span&gt;&lt;span class=&quot;k&quot;&gt;}&lt;/span&gt;&lt;span class=&quot;s2&quot;&gt;/share/hadoop/common/lib/jackson-core-asl-1.9.13.jar&quot;&lt;/span&gt;
&lt;span class=&quot;nv&quot;&gt;CLASSPATH&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;s2&quot;&gt;&quot;&lt;/span&gt;&lt;span class=&quot;k&quot;&gt;${&lt;/span&gt;&lt;span class=&quot;nv&quot;&gt;CLASSPATH&lt;/span&gt;&lt;span class=&quot;k&quot;&gt;}&lt;/span&gt;&lt;span class=&quot;s2&quot;&gt;:&lt;/span&gt;&lt;span class=&quot;k&quot;&gt;${&lt;/span&gt;&lt;span class=&quot;nv&quot;&gt;HADOOP_HOME&lt;/span&gt;&lt;span class=&quot;k&quot;&gt;}&lt;/span&gt;&lt;span class=&quot;s2&quot;&gt;/share/hadoop/common/lib/jackson-mapper-asl-1.9.13.jar&quot;&lt;/span&gt;
&lt;span class=&quot;nb&quot;&gt;export &lt;/span&gt;CLASSPATH
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;Include &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;-Dorg.wildfly.openssl.path&lt;/code&gt; to &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;JAVA_OPTS&lt;/code&gt; in &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;accumulo-env.sh&lt;/code&gt; as shown below. This
java property is an optional performance enhancement for TLS.&lt;/p&gt;

&lt;div class=&quot;language-bash highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;nv&quot;&gt;JAVA_OPTS&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=(&lt;/span&gt;&lt;span class=&quot;s2&quot;&gt;&quot;&lt;/span&gt;&lt;span class=&quot;k&quot;&gt;${&lt;/span&gt;&lt;span class=&quot;nv&quot;&gt;ACCUMULO_JAVA_OPTS&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[@]&lt;/span&gt;&lt;span class=&quot;k&quot;&gt;}&lt;/span&gt;&lt;span class=&quot;s2&quot;&gt;&quot;&lt;/span&gt;
  &lt;span class=&quot;s1&quot;&gt;'-XX:+UseConcMarkSweepGC'&lt;/span&gt;
  &lt;span class=&quot;s1&quot;&gt;'-XX:CMSInitiatingOccupancyFraction=75'&lt;/span&gt;
  &lt;span class=&quot;s1&quot;&gt;'-XX:+CMSClassUnloadingEnabled'&lt;/span&gt;
  &lt;span class=&quot;s1&quot;&gt;'-XX:OnOutOfMemoryError=kill -9 %p'&lt;/span&gt;
  &lt;span class=&quot;s1&quot;&gt;'-XX:-OmitStackTraceInFastThrow'&lt;/span&gt;
  &lt;span class=&quot;s1&quot;&gt;'-Djava.net.preferIPv4Stack=true'&lt;/span&gt;
  &lt;span class=&quot;s1&quot;&gt;'-Dorg.wildfly.openssl.path=/usr/lib64'&lt;/span&gt;
  &lt;span class=&quot;s2&quot;&gt;&quot;-Daccumulo.native.lib.path=&lt;/span&gt;&lt;span class=&quot;k&quot;&gt;${&lt;/span&gt;&lt;span class=&quot;nv&quot;&gt;lib&lt;/span&gt;&lt;span class=&quot;k&quot;&gt;}&lt;/span&gt;&lt;span class=&quot;s2&quot;&gt;/native&quot;&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;)&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;Set the following in &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;accumulo.properties&lt;/code&gt; and then run &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;accumulo init&lt;/code&gt;, but don’t start Accumulo.&lt;/p&gt;

&lt;div class=&quot;language-ini highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;py&quot;&gt;instance.volumes&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;hdfs://&amp;lt;name node&amp;gt;/accumulo&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;After running Accumulo init we need to configure storing write ahead logs in
HDFS.  Set the following in &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;accumulo.properties&lt;/code&gt;.&lt;/p&gt;

&lt;div class=&quot;language-ini highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;py&quot;&gt;instance.volumes&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;hdfs://&amp;lt;namenode&amp;gt;/accumulo,abfss://&amp;lt;file_system&amp;gt;@&amp;lt;storage_account_name&amp;gt;.dfs.core.windows.net/accumulo&lt;/span&gt;
&lt;span class=&quot;py&quot;&gt;general.volume.chooser&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;org.apache.accumulo.server.fs.PreferredVolumeChooser&lt;/span&gt;
&lt;span class=&quot;py&quot;&gt;general.custom.volume.preferred.default&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;abfss://&amp;lt;file_system&amp;gt;@&amp;lt;storage_account_name&amp;gt;.dfs.core.windows.net/accumulo&lt;/span&gt;
&lt;span class=&quot;py&quot;&gt;general.custom.volume.preferred.logger&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;hdfs://&amp;lt;namenode&amp;gt;/accumulo&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;Run &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;accumulo init --add-volumes&lt;/code&gt; to initialize the Azure DLS Gen2 volume.  Doing this
in two steps avoids putting any Accumulo metadata files in Gen2  during init.
Copy &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;accumulo.properties&lt;/code&gt; to all nodes and start Accumulo.&lt;/p&gt;

&lt;p&gt;Individual tables can be configured to store their files in HDFS by setting the
table property &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;table.custom.volume.preferred&lt;/code&gt;.  This should be set for the
metadata table in case it splits using the following Accumulo shell command.&lt;/p&gt;

&lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;config -t accumulo.metadata -s table.custom.volume.preferred=hdfs://&amp;lt;namenode&amp;gt;/accumulo
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;h2 id=&quot;accumulo-example&quot;&gt;Accumulo example&lt;/h2&gt;

&lt;p&gt;The following Accumulo shell session shows an example of writing data to Gen2 and
reading it back.  It also shows scanning the metadata table to verify the data
is stored in Gen2.&lt;/p&gt;

&lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;root@muchos&amp;gt; createtable gen2test
root@muchos gen2test&amp;gt; insert r1 f1 q1 v1
root@muchos gen2test&amp;gt; insert r1 f1 q2 v2
root@muchos gen2test&amp;gt; flush -w
2019-10-16 08:01:00,564 [shell.Shell] INFO : Flush of table gen2test  completed.
root@muchos gen2test&amp;gt; scan
r1 f1:q1 []    v1
r1 f1:q2 []    v2
root@muchos gen2test&amp;gt; scan -t accumulo.metadata -c file
4&amp;lt; file:abfss://&amp;lt;file_system&amp;gt;@&amp;lt;storage_account_name&amp;gt;.dfs.core.windows.net/accumulo/tables/4/default_tablet/F00000gj.rf []    234,2
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;These instructions will help to configure Accumulo to use Azure’s Data Lake Gen2 Storage along with HDFS. With this setup, 
we are able to successfully run the continuos ingest test. Going forward, we’ll experiment more on this space 
with ADLS Gen2 and add/update blog as we come along.&lt;/p&gt;

</description>
        <pubDate>Tue, 15 Oct 2019 00:00:00 +0000</pubDate>
        <link>https://accumulo.apache.org/blog/2019/10/15/accumulo-adlsgen2-notes.html</link>
        <guid isPermaLink="true">https://accumulo.apache.org/blog/2019/10/15/accumulo-adlsgen2-notes.html</guid>
        
        
        <category>blog</category>
        
      </item>
    
  </channel>
</rss>
